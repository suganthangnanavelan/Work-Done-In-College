{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246eeb3-2fc3-4dea-9d6f-f9742c74b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# --------- Classification: IRIS Dataset ---------\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X_cls = iris.data\n",
    "y_cls = iris.target\n",
    "\n",
    "# Split\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler_cls = StandardScaler()\n",
    "X_train_cls = scaler_cls.fit_transform(X_train_cls)\n",
    "X_test_cls = scaler_cls.transform(X_test_cls)\n",
    "\n",
    "# Build MLP model\n",
    "model_cls = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train_cls.shape[1],)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model_cls.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history_cls = model_cls.fit(X_train_cls, y_train_cls, epochs=50, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_cls = np.argmax(model_cls.predict(X_test_cls), axis=1)\n",
    "acc = accuracy_score(y_test_cls, y_pred_cls)\n",
    "print(f\"Classification Accuracy on Iris dataset: {acc:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_cls.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_cls.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Classification Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# --------- Regression: California Housing Dataset ---------\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X_reg = housing.data\n",
    "y_reg = housing.target\n",
    "\n",
    "# Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Build MLP model\n",
    "model_reg = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train_reg.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model_reg.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train\n",
    "history_reg = model_reg.fit(X_train_reg, y_train_reg, epochs=50, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_reg = model_reg.predict(X_test_reg).flatten()\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Regression MSE on California Housing dataset: {mse:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_reg.history['loss'], label='Train Loss')\n",
    "plt.plot(history_reg.history['val_loss'], label='Val Loss')\n",
    "plt.title('Regression Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97be2ce-bb9d-4a5c-be94-a9ac5ba31d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------- Classification: IRIS Dataset ---------\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X_cls = iris.data\n",
    "y_cls = iris.target\n",
    "\n",
    "# Split\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_cls = StandardScaler()\n",
    "X_train_cls = scaler_cls.fit_transform(X_train_cls)\n",
    "X_test_cls = scaler_cls.transform(X_test_cls)\n",
    "\n",
    "# Create and train MLP Classifier\n",
    "mlp_cls = MLPClassifier(hidden_layer_sizes=(16,8), activation='relu', solver='adam', max_iter=300, random_state=42, verbose=True)\n",
    "mlp_cls.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_cls = mlp_cls.predict(X_test_cls)\n",
    "acc = accuracy_score(y_test_cls, y_pred_cls)\n",
    "print(f\"Classification Accuracy on Iris dataset: {acc:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(mlp_cls.loss_curve_)\n",
    "plt.title('Classification Loss Curve (IRIS)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# --------- Regression: California Housing Dataset ---------\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X_reg = housing.data\n",
    "y_reg = housing.target\n",
    "\n",
    "# Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Create and train MLP Regressor\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(32,16), activation='relu', solver='adam', max_iter=300, random_state=42, verbose=True)\n",
    "mlp_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_reg = mlp_reg.predict(X_test_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Regression MSE on California Housing dataset: {mse:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(mlp_reg.loss_curve_)\n",
    "plt.title('Regression Loss Curve (California Housing)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df4a42-e6b7-417e-83a6-032e15d42f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------- Classification: IRIS Dataset ---------\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X_cls = iris.data\n",
    "y_cls = iris.target\n",
    "\n",
    "# Split\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_cls = StandardScaler()\n",
    "X_train_cls = scaler_cls.fit_transform(X_train_cls)\n",
    "X_test_cls = scaler_cls.transform(X_test_cls)\n",
    "\n",
    "# Create and train MLP Classifier\n",
    "mlp_cls = MLPClassifier(hidden_layer_sizes=(16,8), activation='relu', solver='adam', max_iter=300, random_state=42, verbose=True)\n",
    "mlp_cls.fit(X_train_cls, y_train_cls)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_cls = mlp_cls.predict(X_test_cls)\n",
    "acc = accuracy_score(y_test_cls, y_pred_cls)\n",
    "print(f\"Classification Accuracy on Iris dataset: {acc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test_cls, y_pred_cls)\n",
    "\n",
    "# --------- Regression: California Housing Dataset ---------\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "X_reg = housing.data\n",
    "y_reg = housing.target\n",
    "\n",
    "# Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Create and train MLP Regressor\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(32,16), activation='relu', solver='adam', max_iter=300, random_state=42, verbose=True)\n",
    "mlp_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_reg = mlp_reg.predict(X_test_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Regression MSE on California Housing dataset: {mse:.4f}\")\n",
    "\n",
    "# --------- PLOTS ---------\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "# 1. Classification Loss Curve\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(mlp_cls.loss_curve_)\n",
    "plt.title('Classification Loss Curve (IRIS)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "plt.subplot(2,3,2)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix (IRIS)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# 3. Learning Curve for Classification\n",
    "train_sizes_cls, train_scores_cls, val_scores_cls = learning_curve(mlp_cls, X_cls, y_cls, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1,1.0,5))\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(train_sizes_cls, np.mean(train_scores_cls, axis=1), label='Train Accuracy')\n",
    "plt.plot(train_sizes_cls, np.mean(val_scores_cls, axis=1), label='Validation Accuracy')\n",
    "plt.title('Learning Curve (IRIS Classification)')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 4. Regression Loss Curve\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(mlp_reg.loss_curve_)\n",
    "plt.title('Regression Loss Curve (California Housing)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 5. Prediction vs Ground Truth Scatter Plot\n",
    "plt.subplot(2,3,5)\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.7)\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')\n",
    "plt.title('Regression Prediction vs Ground Truth')\n",
    "plt.xlabel('True Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "\n",
    "# 6. Learning Curve for Regression\n",
    "train_sizes_reg, train_scores_reg, val_scores_reg = learning_curve(mlp_reg, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error', train_sizes=np.linspace(0.1,1.0,5))\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(train_sizes_reg, -np.mean(train_scores_reg, axis=1), label='Train MSE')\n",
    "plt.plot(train_sizes_reg, -np.mean(val_scores_reg, axis=1), label='Validation MSE')\n",
    "plt.title('Learning Curve (Housing Regression)')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1b0bb-70b8-4b29-80e6-2b678c17b0a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------- PLOTS ---------\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "# 1. Classification Loss Curve\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(mlp_cls.loss_curve_)\n",
    "plt.title('Classification Loss Curve (IRIS)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "plt.subplot(2,3,2)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix (IRIS)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# 3. Learning Curve for Classification\n",
    "train_sizes_cls, train_scores_cls, val_scores_cls = learning_curve(mlp_cls, X_cls, y_cls, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1,1.0,5))\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(train_sizes_cls, np.mean(train_scores_cls, axis=1), label='Train Accuracy')\n",
    "plt.plot(train_sizes_cls, np.mean(val_scores_cls, axis=1), label='Validation Accuracy')\n",
    "plt.title('Learning Curve (IRIS Classification)')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 4. Regression Loss Curve\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(mlp_reg.loss_curve_)\n",
    "plt.title('Regression Loss Curve (California Housing)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# 5. Prediction vs Ground Truth Scatter Plot\n",
    "plt.subplot(2,3,5)\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.7)\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')\n",
    "plt.title('Regression Prediction vs Ground Truth')\n",
    "plt.xlabel('True Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "\n",
    "# 6. Learning Curve for Regression\n",
    "train_sizes_reg, train_scores_reg, val_scores_reg = learning_curve(mlp_reg, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error', train_sizes=np.linspace(0.1,1.0,5))\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(train_sizes_reg, -np.mean(train_scores_reg, axis=1), label='Train MSE')\n",
    "plt.plot(train_sizes_reg, -np.mean(val_scores_reg, axis=1), label='Validation MSE')\n",
    "plt.title('Learning Curve (Housing Regression)')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aabf3a9-aa7f-4fc4-887a-4a4648d02559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Correct path\n",
    "data_dir = r\"E:\\seg_train\\seg_train\"  # NOT just seg_train\n",
    "\n",
    "# Image parameters\n",
    "img_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Data Generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # MULTICLASS\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # MULTI CLASS\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Plotting Accuracy & Loss\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "val_generator.reset()\n",
    "y_pred = model.predict(val_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = val_generator.classes\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=val_generator.class_indices.keys(), yticklabels=val_generator.class_indices.keys())\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_true, y_pred_classes, target_names=val_generator.class_indices.keys()))\n",
    "\n",
    "# Sample predictions\n",
    "class_labels = list(val_generator.class_indices.keys())\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(12):\n",
    "    img, label = next(val_generator)   # <-- CORRECT way (not .next())\n",
    "    pred = model.predict(img)\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.imshow(img[0])\n",
    "    plt.axis('off')\n",
    "    true_label = class_labels[np.argmax(label[0])]\n",
    "    pred_label = class_labels[np.argmax(pred[0])]\n",
    "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3e2ce-4b14-494d-88b6-221c679f3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r\"E:\\heart_failure_clinical_records_dataset.csv\")\n",
    "\n",
    "# Features and Labels\n",
    "X = data.drop('DEATH_EVENT', axis=1)\n",
    "y = data['DEATH_EVENT']\n",
    "\n",
    "# (Optional) Group the classes to simplify\n",
    "#y = y.apply(lambda q: 0 if q <= 5 else (1 if q == 6 else 2))  # 0: bad, 1: average, 2: good\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create MLP Classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=300, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e12f4-7262-415b-abc2-cd2bdaa91f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
