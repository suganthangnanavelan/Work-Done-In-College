{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 45818,
     "status": "ok",
     "timestamp": 1743487279036,
     "user": {
      "displayName": "Jaison A",
      "userId": "07006398627763032071"
     },
     "user_tz": -330
    },
    "id": "TNO6tqQ977Mr",
    "outputId": "67fbc432-5c26-4e05-8928-06c959724944"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('/content/heart_disease_uci.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "numeric_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "# Impute missing values\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "df[numeric_cols] = imputer_num.fit_transform(df[numeric_cols])\n",
    "\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Encode categorical variables\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Target variable\n",
    "df['target'] = (df['num'] > 0).astype(int)\n",
    "\n",
    "# Select features and target\n",
    "features = numeric_cols + categorical_cols\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "## Experiment 10: Regularization Techniques\n",
    "\n",
    "# Best architecture from Experiment 9\n",
    "best_architecture = (20, 10)\n",
    "\n",
    "def evaluate_regularized_model(regularization_params):\n",
    "    # Remove 'name' key if present\n",
    "    params = {k:v for k,v in regularization_params.items() if k != \"name\"}\n",
    "\n",
    "    # For dropout simulation, we'll adjust alpha instead\n",
    "    if \"dropout\" in params:\n",
    "        # In scikit-learn, we can simulate dropout effect by increasing alpha\n",
    "        params[\"alpha\"] = params.pop(\"dropout\") * 10  # Scaling factor\n",
    "        params[\"solver\"] = \"adam\"  # Adam works better with this approach\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=best_architecture,\n",
    "                         activation='relu',\n",
    "                         max_iter=500,\n",
    "                         random_state=42,\n",
    "                         **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy, model.loss_curve_\n",
    "\n",
    "# Test different regularization techniques\n",
    "regularization_tests = [\n",
    "    {\"name\": \"No Regularization\", \"alpha\": 0},\n",
    "    {\"alpha\": 0.0001, \"name\": \"L2 (alpha=0.0001)\"},\n",
    "    {\"alpha\": 0.001, \"name\": \"L2 (alpha=0.001)\"},\n",
    "    {\"alpha\": 0.01, \"name\": \"L2 (alpha=0.01)\"},\n",
    "    {\"early_stopping\": True, \"validation_fraction\": 0.2, \"name\": \"Early Stopping\"},\n",
    "    {\"alpha\": 0.001, \"early_stopping\": True, \"name\": \"L2 + Early Stopping\"},\n",
    "    {\"dropout\": 0.2, \"name\": \"Simulated Dropout (alpha=2)\"},\n",
    "    {\"alpha\": 0.001, \"early_stopping\": True, \"solver\": \"adam\", \"name\": \"L2 + Early Stop + Adam\"}\n",
    "]\n",
    "\n",
    "results = []\n",
    "loss_curves = []\n",
    "\n",
    "for reg_test in regularization_tests:\n",
    "    accuracy, loss_curve = evaluate_regularized_model(reg_test)\n",
    "    results.append({\n",
    "        \"Regularization\": reg_test[\"name\"],\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "    loss_curves.append((reg_test[\"name\"], loss_curve))\n",
    "\n",
    "    print(f\"\\nRegularization: {reg_test['name']}\")\n",
    "    print(\"Classification Report:\")\n",
    "    model_params = {k:v for k,v in reg_test.items() if k not in [\"name\", \"dropout\"]}\n",
    "    if \"dropout\" in reg_test:\n",
    "        model_params[\"alpha\"] = reg_test[\"dropout\"] * 10\n",
    "    model = MLPClassifier(hidden_layer_sizes=st_architecture,\n",
    "                         activation='relu',\n",
    "                         max_iter=500,\n",
    "                         random_state=42,\n",
    "                         **model_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of Regularization Results:\")\n",
    "print(results_df.sort_values(by=\"Accuracy\", ascending=False))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(results_df['Regularization'], results_df['Accuracy'], color='lightgreen')\n",
    "plt.title('Test Accuracy by Regularization Technique')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.xlim(0.7, 0.9)\n",
    "\n",
    "# Training loss curves\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, curve in loss_curves:\n",
    "    plt.plot(curve, label=name)\n",
    "plt.title('Training Loss with Regularization')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNefA49SOjY7QnPMPjmtmQE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
