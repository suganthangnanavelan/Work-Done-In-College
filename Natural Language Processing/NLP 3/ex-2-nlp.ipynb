{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f37e273-c9bd-4447-b164-ae3d72d60c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Rank | Bigram                      |   Frequency |   Mean Probability (μ-value) |\n",
      "|--------+-----------------------------+-------------+------------------------------|\n",
      "|      1 | ('data', 'science')         |          15 |                     0.020243 |\n",
      "|      2 | ('data', 'processing')      |           7 |                     0.009447 |\n",
      "|      3 | ('predictive', 'analytics') |           5 |                     0.006748 |\n",
      "|      4 | ('data', 'visualization')   |           5 |                     0.006748 |\n",
      "|      5 | ('ai', 'data')              |           4 |                     0.005398 |\n",
      "|      6 | ('ai', 'algorithms')        |           4 |                     0.005398 |\n",
      "|      7 | ('data', 'cleaning')        |           4 |                     0.005398 |\n",
      "|      8 | ('cleaning', 'preparation') |           4 |                     0.005398 |\n",
      "|      9 | ('natural', 'language')     |           4 |                     0.005398 |\n",
      "|     10 | ('language', 'processing')  |           4 |                     0.005398 |\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from tabulate import tabulate\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Preprocesses the text by tokenizing, removing punctuation and stopwords.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "def calculate_mean_probability(bigram_freq: FreqDist, total_bigrams: int) -> dict:\n",
    "    \"\"\"Calculates the mean probability (μ-value) of each bigram.\"\"\"\n",
    "    mean_probabilities = {bigram: freq / total_bigrams for bigram, freq in bigram_freq.items()}\n",
    "    return mean_probabilities\n",
    "\n",
    "def main():\n",
    "    # Download required NLTK data\n",
    "    # nltk.download('punkt')\n",
    "    # nltk.download('stopwords')\n",
    "\n",
    "    # Load text\n",
    "    with open('text3.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Preprocess text\n",
    "    words = preprocess_text(text)\n",
    "\n",
    "    # Calculate word frequency distribution\n",
    "    fdist = FreqDist(words)\n",
    "\n",
    "    # Calculate bigrams and their frequencies\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    bigram_freq = FreqDist(bigrams)\n",
    "\n",
    "    # Calculate mean probability (μ-value) for each bigram\n",
    "    mean_probabilities = calculate_mean_probability(bigram_freq, len(bigrams))\n",
    "\n",
    "    # Sort collocations by mean probability\n",
    "    collocations = sorted(mean_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    '''# Calculate mean of bigram frequencies\n",
    "    total_bigram_freq = sum(bigram_freq.values())\n",
    "    mean_bigram_freq = total_bigram_freq / len(bigram_freq)\n",
    "\n",
    "    # Print mean bigram frequency\n",
    "    print(f\"Mean Bigram Frequency: {mean_bigram_freq:.2f}\\n\")'''\n",
    "\n",
    "    # Print top N collocations with their frequencies and mean probabilities\n",
    "    N = 10\n",
    "    headers = [\"Rank\", \"Bigram\", \"Frequency\", \"Mean Probability (μ-value)\"]\n",
    "    table = []\n",
    "    for i, (bigram, mean_prob) in enumerate(collocations[:N]):\n",
    "        table.append([i+1, bigram, bigram_freq[bigram], f\"{mean_prob:.6f}\"])\n",
    "    print(tabulate(table, headers, tablefmt=\"orgtbl\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba8ff30-565b-471a-a161-e47e0eb72cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Bigram Frequency: 1.17\n",
      "\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|   Rank | Bigram                         |   Frequency |   Mean Prob(μ) |   t-Statistic |   p-Value(t-Test) |   Chi Square |   p-Value(Chi-Square) |\n",
      "+========+================================+=============+================+===============+===================+==============+=======================+\n",
      "|      1 | ('impact', 'artificial')       |           2 |       0.002699 |        1.4152 |            0.1574 |     183.747  |                0      |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      2 | ('artificial', 'intelligence') |           3 |       0.004049 |        1.7344 |            0.0833 |     307.077  |                0      |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      3 | ('intelligence', 'data')       |           3 |       0.004049 |        1.7344 |            0.0833 |      12.4097 |                0.0004 |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      4 | ('data', 'science')            |          15 |       0.020243 |        3.9101 |            0.0001 |     167.483  |                0      |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      5 | ('science', 'introduction')    |           1 |       0.00135  |        1      |            0.3176 |      11.6208 |                0.0007 |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      6 | ('introduction', 'recent')     |           1 |       0.00135  |        1      |            0.3176 |     184.75   |                0      |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      7 | ('recent', 'years')            |           1 |       0.00135  |        1      |            0.3176 |     184.75   |                0      |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      8 | ('years', 'convergence')       |           1 |       0.00135  |        1      |            0.3176 |     184.75   |                0      |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|      9 | ('convergence', 'artificial')  |           1 |       0.00135  |        1      |            0.3176 |      61.0835 |                0      |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n",
      "|     10 | ('intelligence', 'ai')         |           1 |       0.00135  |        1      |            0.3176 |       0.4959 |                0.4813 |\n",
      "+--------+--------------------------------+-------------+----------------+---------------+-------------------+--------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import chi2_contingency, ttest_1samp\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text: str) -> list[str]:\n",
    "    \"\"\"Preprocesses the text by tokenizing, removing punctuation and stopwords.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "def calculate_mean_probability(bigram_freq: FreqDist, total_bigrams: int) -> dict:\n",
    "    \"\"\"Calculates the mean probability (μ-value) of each bigram.\"\"\"\n",
    "    mean_probabilities = {bigram: freq / total_bigrams for bigram, freq in bigram_freq.items()}\n",
    "    return mean_probabilities\n",
    "\n",
    "def perform_statistical_tests(bigram_freq: FreqDist, word_freq: FreqDist, total_bigrams: int):\n",
    "    \"\"\"Perform t-test and chi-square test for each bigram.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for bigram, observed_freq in bigram_freq.items():\n",
    "        word1, word2 = bigram\n",
    "        freq_w1 = word_freq.get(word1, 0)\n",
    "        freq_w2 = word_freq.get(word2, 0)\n",
    "        \n",
    "        # Expected frequency for the bigram assuming independence\n",
    "        expected_freq = (freq_w1 * freq_w2) / total_bigrams\n",
    "        \n",
    "        # Chi-square test\n",
    "        observed = np.array([\n",
    "            [observed_freq, freq_w1 - observed_freq],\n",
    "            [freq_w2 - observed_freq, total_bigrams - (freq_w1 + freq_w2 - observed_freq)]\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            chi2_stat, p_value_chi2, dof, ex = chi2_contingency(observed)\n",
    "        except ValueError:\n",
    "            chi2_stat, p_value_chi2 = np.nan, np.nan\n",
    "        \n",
    "        # Generate sample data to perform t-test\n",
    "        sample_data = [observed_freq] * observed_freq + [expected_freq] * (total_bigrams - observed_freq)\n",
    "        \n",
    "        # Perform one-sample t-test\n",
    "        t_stat, p_value_t = ttest_1samp(sample_data, expected_freq)\n",
    "        \n",
    "        results.append((bigram, observed_freq, t_stat, p_value_t, chi2_stat, p_value_chi2))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Download required NLTK data\n",
    "    # nltk.download('punkt')\n",
    "    # nltk.download('stopwords')\n",
    "\n",
    "    # Load text\n",
    "    with open(\"text3.txt\", 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Preprocess text\n",
    "    words = preprocess_text(text)\n",
    "\n",
    "    # Calculate word frequency distribution\n",
    "    word_freq = FreqDist(words)\n",
    "\n",
    "    # Calculate bigrams and their frequencies\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    bigram_freq = FreqDist(bigrams)\n",
    "\n",
    "    # Calculate mean probability (μ-value) for each bigram\n",
    "    mean_probabilities = calculate_mean_probability(bigram_freq, len(bigrams))\n",
    "\n",
    "    # Sort collocations by mean probability\n",
    "    collocations = sorted(mean_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate mean of bigram frequencies\n",
    "    total_bigram_freq = sum(bigram_freq.values())\n",
    "    mean_bigram_freq = total_bigram_freq / len(bigram_freq)\n",
    "\n",
    "    # Print mean bigram frequency\n",
    "    print(f\"Mean Bigram Frequency: {mean_bigram_freq:.2f}\\n\")\n",
    "\n",
    "    # Perform statistical tests for each bigram\n",
    "    results = perform_statistical_tests(bigram_freq, word_freq, len(bigrams))\n",
    "\n",
    "    # Print top N collocations with their frequencies and mean probabilities\n",
    "    N = 10\n",
    "    headers = [\"Rank\", \"Bigram\", \"Frequency\", \"Mean Prob(μ)\", \"t-Statistic\", \"p-Value(t-Test)\", \"Chi Square\", \"p-Value(Chi-Square)\"]\n",
    "    table = []\n",
    "    for i, (bigram, observed_freq, t_stat, p_value_t, chi2_stat, p_value_chi2) in enumerate(results[:N]):\n",
    "        table.append([\n",
    "            i + 1,\n",
    "            bigram,\n",
    "            observed_freq,\n",
    "            f\"{mean_probabilities.get(bigram, 0):.6f}\",\n",
    "            f\"{t_stat:.4f}\" if not np.isnan(t_stat) else \"NaN\",\n",
    "            f\"{p_value_t:.4f}\" if not np.isnan(p_value_t) else \"NaN\",\n",
    "            f\"{chi2_stat:.4f}\" if not np.isnan(chi2_stat) else \"NaN\",\n",
    "            f\"{p_value_chi2:.4f}\" if not np.isnan(p_value_chi2) else \"NaN\"\n",
    "        ])\n",
    "    print(tabulate(table, headers, tablefmt=\"grid\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee8280-0449-4fb4-846e-3128f29a09cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
