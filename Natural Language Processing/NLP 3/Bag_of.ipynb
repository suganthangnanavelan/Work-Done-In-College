{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d785cbc0-ca92-485c-9ddd-22f29c9e7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ability: 2\n",
      "accounting: 1\n",
      "accuracy: 2\n",
      "accurate: 1\n",
      "accurately: 1\n",
      "across: 1\n",
      "action: 1\n",
      "actionable: 1\n",
      "actions: 1\n",
      "activities: 1\n",
      "activity: 1\n",
      "additionally: 1\n",
      "addressed: 1\n",
      "addressing: 2\n",
      "adjusting: 1\n",
      "advanced: 5\n",
      "advancement: 1\n",
      "advancing: 1\n",
      "age: 1\n",
      "ai: 34\n",
      "alert: 1\n",
      "algorithm: 6\n",
      "algorithms: 3\n",
      "allow: 1\n",
      "allowing: 2\n",
      "also: 2\n",
      "amount: 2\n",
      "analysis: 8\n",
      "analytical: 1\n",
      "analytics: 5\n",
      "analyze: 5\n",
      "analyzing: 3\n",
      "anomaly: 1\n",
      "application: 1\n",
      "applications: 2\n",
      "area: 1\n",
      "artificial: 3\n",
      "aspect: 1\n",
      "ass: 1\n",
      "automated: 1\n",
      "automatically: 2\n",
      "automating: 2\n",
      "automation: 2\n",
      "autonomous: 2\n",
      "available: 1\n",
      "aware: 1\n",
      "bandwidth: 1\n",
      "based: 2\n",
      "becomes: 1\n",
      "benefit: 1\n",
      "bias: 4\n",
      "brings: 1\n",
      "brought: 1\n",
      "business: 1\n",
      "capabilities: 3\n",
      "capability: 3\n",
      "care: 1\n",
      "centralized: 1\n",
      "chain: 1\n",
      "characteristic: 1\n",
      "chart: 1\n",
      "cleaning: 4\n",
      "clinical: 1\n",
      "closer: 1\n",
      "cloud: 1\n",
      "combination: 1\n",
      "competitive: 1\n",
      "complex: 2\n",
      "component: 1\n",
      "computer: 3\n",
      "computing: 2\n",
      "concerns: 3\n",
      "conclusion: 1\n",
      "constitutes: 1\n",
      "continues: 1\n",
      "continuously: 1\n",
      "convergence: 1\n",
      "core: 1\n",
      "correct: 1\n",
      "course: 1\n",
      "create: 1\n",
      "credit: 1\n",
      "critical: 1\n",
      "crucial: 3\n",
      "customer: 2\n",
      "cybersecurity: 1\n",
      "dashboard: 1\n",
      "data: 62\n",
      "datasets: 3\n",
      "decision: 11\n",
      "decisions: 2\n",
      "deep: 1\n",
      "deeper: 1\n",
      "demand: 2\n",
      "deployed: 2\n",
      "designed: 1\n",
      "detect: 3\n",
      "developing: 1\n",
      "development: 1\n",
      "device: 1\n",
      "diagnosis: 1\n",
      "digital: 1\n",
      "discriminatory: 1\n",
      "disruptions: 1\n",
      "diverse: 1\n",
      "drive: 2\n",
      "driven: 7\n",
      "dynamically: 1\n",
      "early: 1\n",
      "edge: 3\n",
      "efficiency: 3\n",
      "effort: 1\n",
      "efforts: 1\n",
      "embracing: 1\n",
      "enabled: 2\n",
      "enabling: 6\n",
      "enhanced: 1\n",
      "enhancing: 4\n",
      "ensure: 2\n",
      "ensuring: 2\n",
      "entity: 1\n",
      "errors: 1\n",
      "essay: 1\n",
      "essential: 2\n",
      "established: 1\n",
      "ethical: 5\n",
      "evaluate: 1\n",
      "evolve: 1\n",
      "example: 3\n",
      "executive: 1\n",
      "explainable: 1\n",
      "explore: 1\n",
      "explores: 1\n",
      "extracting: 4\n",
      "facilitated: 2\n",
      "facilitating: 1\n",
      "fairness: 1\n",
      "far: 1\n",
      "features: 1\n",
      "field: 1\n",
      "fields: 1\n",
      "filter: 1\n",
      "finance: 2\n",
      "financial: 2\n",
      "fluctuations: 1\n",
      "focus: 3\n",
      "forecast: 1\n",
      "forecasting: 1\n",
      "form: 1\n",
      "format: 1\n",
      "fraudulent: 2\n",
      "full: 1\n",
      "fundamentally: 1\n",
      "future: 1\n",
      "generate: 1\n",
      "generated: 3\n",
      "generation: 1\n",
      "govern: 1\n",
      "graphical: 1\n",
      "graphs: 1\n",
      "guideline: 1\n",
      "handle: 1\n",
      "harness: 1\n",
      "healthcare: 4\n",
      "hidden: 1\n",
      "higher: 1\n",
      "highlighting: 2\n",
      "historical: 1\n",
      "human: 4\n",
      "identify: 3\n",
      "identifying: 2\n",
      "image: 1\n",
      "immediate: 1\n",
      "impact: 3\n",
      "impacted: 1\n",
      "implementing: 1\n",
      "important: 1\n",
      "impossible: 1\n",
      "improve: 1\n",
      "improved: 2\n",
      "improving: 1\n",
      "inadvertently: 1\n",
      "include: 1\n",
      "inconsistencies: 1\n",
      "increasingly: 1\n",
      "industrial: 1\n",
      "industries: 1\n",
      "industry: 1\n",
      "inform: 1\n",
      "information: 1\n",
      "informed: 2\n",
      "innovation: 2\n",
      "insight: 3\n",
      "insights: 1\n",
      "instance: 1\n",
      "integration: 1\n",
      "intelligence: 5\n",
      "interaction: 1\n",
      "interactive: 2\n",
      "introduction: 1\n",
      "intuitive: 1\n",
      "invaluable: 1\n",
      "inventory: 2\n",
      "investment: 2\n",
      "involves: 1\n",
      "involving: 1\n",
      "issue: 1\n",
      "key: 2\n",
      "knowledge: 1\n",
      "language: 5\n",
      "large: 1\n",
      "latency: 1\n",
      "lead: 1\n",
      "leading: 2\n",
      "learn: 1\n",
      "learning: 4\n",
      "level: 1\n",
      "levels: 1\n",
      "leveraging: 2\n",
      "like: 3\n",
      "locally: 1\n",
      "machine: 3\n",
      "make: 3\n",
      "making: 6\n",
      "management: 2\n",
      "manner: 1\n",
      "manual: 1\n",
      "manually: 1\n",
      "many: 1\n",
      "market: 1\n",
      "marketing: 2\n",
      "massive: 1\n",
      "meaningful: 1\n",
      "media: 1\n",
      "medical: 1\n",
      "medium: 1\n",
      "method: 1\n",
      "mimic: 1\n",
      "missing: 1\n",
      "mitigate: 1\n",
      "ml: 1\n",
      "model: 3\n",
      "models: 2\n",
      "monitoring: 2\n",
      "more: 1\n",
      "multiple: 1\n",
      "natural: 4\n",
      "navigation: 1\n",
      "networks: 1\n",
      "neural: 1\n",
      "new: 1\n",
      "nlp: 6\n",
      "note: 1\n",
      "numerous: 2\n",
      "object: 1\n",
      "often: 2\n",
      "one: 1\n",
      "operational: 1\n",
      "optimal: 1\n",
      "optimize: 3\n",
      "organization: 3\n",
      "outcome: 1\n",
      "outcomes: 2\n",
      "outliers: 1\n",
      "papers: 1\n",
      "parameter: 1\n",
      "particularly: 3\n",
      "patient: 2\n",
      "pattern: 1\n",
      "patterns: 1\n",
      "perform: 1\n",
      "perpetuate: 1\n",
      "personalize: 1\n",
      "personalized: 1\n",
      "plans: 1\n",
      "platform: 1\n",
      "portfolios: 1\n",
      "portion: 2\n",
      "potential: 2\n",
      "powered: 4\n",
      "powerful: 1\n",
      "predict: 3\n",
      "predictive: 6\n",
      "preparation: 4\n",
      "present: 2\n",
      "primary: 1\n",
      "process: 5\n",
      "processes: 3\n",
      "processing: 13\n",
      "profound: 1\n",
      "project: 1\n",
      "provide: 1\n",
      "providing: 1\n",
      "quick: 1\n",
      "quickly: 1\n",
      "raise: 1\n",
      "reaching: 1\n",
      "real: 7\n",
      "recent: 1\n",
      "recognition: 1\n",
      "recommend: 1\n",
      "reduces: 1\n",
      "reducing: 2\n",
      "regression: 1\n",
      "regulation: 1\n",
      "relationship: 1\n",
      "relevant: 1\n",
      "relying: 1\n",
      "representations: 1\n",
      "representative: 1\n",
      "required: 1\n",
      "requirements: 1\n",
      "research: 2\n",
      "respect: 1\n",
      "response: 1\n",
      "responsible: 1\n",
      "retail: 1\n",
      "revolutionized: 1\n",
      "right: 1\n",
      "risk: 1\n",
      "risks: 1\n",
      "safety: 1\n",
      "sale: 1\n",
      "scenario: 1\n",
      "scenarios: 1\n",
      "science: 15\n",
      "scientist: 1\n",
      "second: 1\n",
      "sector: 1\n",
      "seeking: 1\n",
      "sensors: 1\n",
      "sentiment: 2\n",
      "servers: 1\n",
      "significant: 3\n",
      "significantly: 2\n",
      "similarly: 1\n",
      "social: 2\n",
      "societal: 1\n",
      "sophisticated: 1\n",
      "source: 1\n",
      "sources: 1\n",
      "speed: 1\n",
      "spent: 1\n",
      "split: 1\n",
      "stakeholder: 1\n",
      "stay: 1\n",
      "step: 1\n",
      "strategies: 2\n",
      "stream: 2\n",
      "streaming: 1\n",
      "structured: 1\n",
      "struggle: 1\n",
      "subfield: 1\n",
      "suitable: 1\n",
      "summarization: 1\n",
      "supply: 1\n",
      "support: 2\n",
      "synergy: 1\n",
      "system: 3\n",
      "task: 2\n",
      "tasks: 1\n",
      "technique: 3\n",
      "techniques: 1\n",
      "technology: 1\n",
      "text: 3\n",
      "time: 10\n",
      "today: 2\n",
      "together: 1\n",
      "tool: 2\n",
      "trading: 1\n",
      "traditional: 1\n",
      "training: 2\n",
      "transformation: 1\n",
      "transformed: 4\n",
      "transforming: 2\n",
      "transparent: 1\n",
      "treatment: 1\n",
      "trees: 1\n",
      "trend: 4\n",
      "trends: 1\n",
      "triggering: 1\n",
      "uncover: 1\n",
      "understand: 1\n",
      "understanding: 1\n",
      "unfair: 1\n",
      "unprecedented: 1\n",
      "unstructured: 2\n",
      "use: 2\n",
      "user: 1\n",
      "valuable: 3\n",
      "value: 1\n",
      "values: 1\n",
      "various: 2\n",
      "vast: 2\n",
      "vehicles: 2\n",
      "videos: 1\n",
      "vision: 2\n",
      "visualization: 7\n",
      "vital: 1\n",
      "way: 1\n",
      "within: 1\n",
      "without: 1\n",
      "workflow: 1\n",
      "world: 1\n",
      "would: 1\n",
      "years: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the text data from the file\n",
    "with open('text3.txt', 'r') as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# Tokenize the text data\n",
    "tokens = text_data.split()\n",
    "\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Perform stemming or lemmatization (optional)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Join the tokens back into a string\n",
    "text_data = ' '.join(tokens)\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the text data and transform it into a matrix\n",
    "X = vectorizer.fit_transform([text_data])\n",
    "\n",
    "# Get the feature names (i.e., the unique words in the document)\n",
    "feature_names = vectorizer.get_feature_names_out()  # Use get_feature_names_out() instead\n",
    "\n",
    "# Print the feature names and their corresponding frequencies\n",
    "for feature, freq in zip(feature_names, X.toarray()[0]):\n",
    "    print(f\"{feature}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46eeb633-6817-4852-a61e-7eccc1f018db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Bag of Words model accuracy: 1.000\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_6\" is incompatible with the layer: expected axis -1 of input shape to have value 5000, but received input with shape (None, 374)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 374), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m tfidf_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m tfidf_model\u001b[38;5;241m.\u001b[39mfit(tfidf_train, train_labels_onehot, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(tfidf_test, test_labels_onehot))\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     95\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m tfidf_model\u001b[38;5;241m.\u001b[39mevaluate(tfidf_test, test_labels_onehot)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_6\" is incompatible with the layer: expected axis -1 of input shape to have value 5000, but received input with shape (None, 374)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 374), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "with open('text3.txt', 'r') as f:\n",
    "    text_data = f.readlines()\n",
    "\n",
    "# Split the data into input text and labels\n",
    "# Split the data into input text and labels\n",
    "text = []\n",
    "labels = []\n",
    "label_map = {}  # Create a label map to store unique labels\n",
    "label_index = 0  # Initialize a label index\n",
    "for line in text_data:\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) > 1:\n",
    "        text.append(parts[0])\n",
    "        label = parts[1].strip()\n",
    "    else:\n",
    "        text.append(parts[0])\n",
    "        label = 'default_label'  # Replace with your default label\n",
    "\n",
    "    if label not in label_map:\n",
    "        label_map[label] = label_index\n",
    "        label_index += 1\n",
    "\n",
    "    labels.append(label_map[label])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(text, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(label_map)\n",
    "train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels_onehot = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Create a tokenizer to split the text into words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# Convert the text data into sequences of words\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "max_length = 200\n",
    "padded_train = pad_sequences(train_sequences, maxlen=max_length)\n",
    "padded_test = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(set(labels))\n",
    "train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels_onehot = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Define the Bag of Words model\n",
    "bow_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 64, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "bow_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "bow_model.fit(padded_train, train_labels_onehot, epochs=10, batch_size=32, validation_data=(padded_test, test_labels_onehot))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = bow_model.evaluate(padded_test, test_labels_onehot)\n",
    "print(f'Bag of Words model accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Define the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_text)\n",
    "tfidf_test = tfidf_vectorizer.transform(test_text)\n",
    "\n",
    "# Define the TF-IDF model\n",
    "tfidf_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(5000,)),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tfidf_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "tfidf_model.fit(tfidf_train, train_labels_onehot, epochs=10, batch_size=32, validation_data=(tfidf_test, test_labels_onehot))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = tfidf_model.evaluate(tfidf_test, test_labels_onehot)\n",
    "print(f'TF-IDF model accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b1dcc9-9060-430f-a3c1-6fa739ccd8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Bag of Words model accuracy: 1.000\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node RaggedGather_1/RaggedGather defined at (most recent call last):\n<stack traces unavailable>\nindices[11] = 36 is not in [0, 36)\n\t [[{{node RaggedGather_1/RaggedGather}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_8973]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m tfidf_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m tfidf_model\u001b[38;5;241m.\u001b[39mfit(tfidf_train, train_labels_onehot, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(tfidf_test, test_labels_onehot))\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     89\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m tfidf_model\u001b[38;5;241m.\u001b[39mevaluate(tfidf_test, test_labels_onehot)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node RaggedGather_1/RaggedGather defined at (most recent call last):\n<stack traces unavailable>\nindices[11] = 36 is not in [0, 36)\n\t [[{{node RaggedGather_1/RaggedGather}}]]\n\t [[IteratorGetNext]] [Op:__inference_one_step_on_iterator_8973]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the dataset\n",
    "with open('text3.txt', 'r') as f:\n",
    "    text_data = f.readlines()\n",
    "\n",
    "# Split the data into input text and labels\n",
    "text = []\n",
    "labels = []\n",
    "label_map = {}  # Create a label map to store unique labels\n",
    "label_index = 0  # Initialize a label index\n",
    "for line in text_data:\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) > 1:\n",
    "        text.append(parts[0])\n",
    "        label = parts[1].strip()\n",
    "    else:\n",
    "        text.append(parts[0])\n",
    "        label = 'default_label'  # Replace with your default label\n",
    "\n",
    "    if label not in label_map:\n",
    "        label_map[label] = label_index\n",
    "        label_index += 1\n",
    "\n",
    "    labels.append(label_map[label])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(text, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a tokenizer to split the text into words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# Convert the text data into sequences of words\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "max_length = 200\n",
    "padded_train = pad_sequences(train_sequences, maxlen=max_length)\n",
    "padded_test = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(label_map)\n",
    "train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels_onehot = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Define the Bag of Words model\n",
    "bow_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 64, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "bow_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "bow_model.fit(padded_train, train_labels_onehot, epochs=10, batch_size=32, validation_data=(padded_test, test_labels_onehot))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = bow_model.evaluate(padded_test, test_labels_onehot)\n",
    "print(f'Bag of Words model accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Define the TF-IDF model\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=374)  # Adjusted max_features to match the shape of tfidf_train\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_text)\n",
    "tfidf_test = tfidf_vectorizer.transform(test_text)\n",
    "\n",
    "# Define the TF-IDF model\n",
    "tfidf_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(374,)),  # Adjusted input shape to match the shape of tfidf_train\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tfidf_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "tfidf_model.fit(tfidf_train, train_labels_onehot, epochs=10, batch_size=32, validation_data=(tfidf_test, test_labels_onehot))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = tfidf_model.evaluate(tfidf_test, test_labels_onehot)\n",
    "print(f'TF-IDF model accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Word2Vec Model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the dataset\n",
    "with open('text3.txt', 'r') as f:\n",
    "    text_data = f.readlines()\n",
    "\n",
    "# Split the data into input text\n",
    "text = [line.split('\\t')[0] for line in text_data]\n",
    "\n",
    "# Split the text into words\n",
    "words = [line.split() for line in text]\n",
    "\n",
    "# Create a Word2Vec model\n",
    "model = Word2Vec(words, size=100, window=5, min_count=1)\n",
    "\n",
    "# Get the word vectors\n",
    "word_vectors = model.wv\n",
    "\n",
    "# Explore semantic similarity between words\n",
    "print(word_vectors.similarity('word1', 'word2'))  # Replace 'word1' and 'word2' with the words you want to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f18e07-76bd-4a44-8882-d8187e5c017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " ['abil' 'account' 'accur' 'accuraci' 'across' 'action' 'activ' 'addit'\n",
      " 'address' 'adjust' 'advanc' 'age' 'ai' 'alert' 'algorithm' 'allow' 'also'\n",
      " 'amount' 'analysi' 'analyt' 'analyz' 'anomali' 'applic' 'area' 'artifici'\n",
      " 'aspect' 'ass' 'autom' 'automat' 'autonom' 'avail' 'awar' 'bandwidth'\n",
      " 'base' 'becom' 'benefit' 'bia' 'bring' 'brought' 'busi' 'capabl' 'care'\n",
      " 'central' 'chain' 'characterist' 'chart' 'clean' 'clinic' 'closer'\n",
      " 'cloud' 'combin' 'competit' 'complex' 'compon' 'comput' 'concern'\n",
      " 'conclus' 'constitut' 'continu' 'converg' 'core' 'correct' 'cours'\n",
      " 'creat' 'credit' 'critic' 'crucial' 'custom' 'cybersecur' 'dashboard'\n",
      " 'data' 'dataset' 'decis' 'decision' 'deep' 'deeper' 'demand' 'deploy'\n",
      " 'design' 'detect' 'develop' 'devic' 'diagnosi' 'digit' 'discriminatori'\n",
      " 'disrupt' 'divers' 'drive' 'driven' 'dynam' 'earli' 'edg' 'effici'\n",
      " 'effort' 'embrac' 'enabl' 'enhanc' 'ensur' 'entiti' 'error' 'essay'\n",
      " 'essenti' 'establish' 'ethic' 'evalu' 'evolv' 'exampl' 'execut' 'explain'\n",
      " 'explor' 'extract' 'facilit' 'fairness' 'far' 'featur' 'field' 'filter'\n",
      " 'financ' 'financi' 'fluctuat' 'focu' 'forecast' 'form' 'format' 'fraudul'\n",
      " 'full' 'fundament' 'futur' 'gener' 'govern' 'graph' 'graphic' 'guidelin'\n",
      " 'handl' 'har' 'healthcar' 'hidden' 'higher' 'highlight' 'histor' 'human'\n",
      " 'identifi' 'imag' 'immedi' 'impact' 'implement' 'import' 'imposs'\n",
      " 'improv' 'inadvert' 'includ' 'inconsist' 'increasingli' 'industri'\n",
      " 'inform' 'innov' 'insight' 'instanc' 'integr' 'intellig' 'interact'\n",
      " 'introduct' 'intuit' 'invalu' 'inventori' 'invest' 'involv' 'issu' 'key'\n",
      " 'knowledg' 'languag' 'larg' 'latenc' 'lead' 'learn' 'level' 'leverag'\n",
      " 'like' 'local' 'machin' 'mak' 'make' 'manag' 'mani' 'manner' 'manual'\n",
      " 'market' 'massiv' 'meaning' 'medic' 'medium' 'method' 'mimic' 'miss'\n",
      " 'mitig' 'ml' 'model' 'monitor' 'multipl' 'natur' 'navig' 'network'\n",
      " 'neural' 'new' 'nlp' 'note' 'numer' 'object' 'often' 'one' 'oper' 'optim'\n",
      " 'organ' 'outcom' 'outlier' 'paper' 'paramet' 'particularli' 'patient'\n",
      " 'pattern' 'perform' 'perpetu' 'person' 'plan' 'platform' 'portfolio'\n",
      " 'portion' 'potenti' 'pow' 'power' 'predict' 'prepar' 'present' 'primari'\n",
      " 'process' 'profound' 'project' 'provid' 'quick' 'quickli' 'rais' 'reach'\n",
      " 'real' 'recent' 'recognit' 'recommend' 'reduc' 'regress' 'regul'\n",
      " 'relationship' 'relev' 'reli' 'repres' 'represent' 'requir' 'research'\n",
      " 'respect' 'respons' 'retail' 'revolution' 'right' 'risk' 'safeti' 'sale'\n",
      " 'scenario' 'scienc' 'scientist' 'second' 'sector' 'seek' 'sensor'\n",
      " 'sentiment' 'server' 'signific' 'significantli' 'similarli' 'social'\n",
      " 'societ' 'sophist' 'sourc' 'speed' 'spent' 'split' 'stakehold' 'stay'\n",
      " 'step' 'strategi' 'stream' 'structur' 'struggl' 'subfield' 'suitabl'\n",
      " 'summar' 'suppli' 'support' 'synergi' 'system' 'task' 'techniqu'\n",
      " 'technolog' 'text' 'tim' 'time' 'today' 'togeth' 'tool' 'trade' 'tradit'\n",
      " 'train' 'transform' 'transpar' 'treatment' 'tree' 'trend' 'trigger'\n",
      " 'uncov' 'understand' 'unfair' 'unpreced' 'unstructur' 'use' 'user' 'valu'\n",
      " 'valuabl' 'variou' 'vast' 'vehicl' 'video' 'vision' 'visual' 'vital'\n",
      " 'way' 'within' 'without' 'workflow' 'world' 'would' 'year']\n",
      "\n",
      "Bag of Words (Vectorization):\n",
      " [[ 2  1  2  2  1  3  2  1  3  1  7  1 34  1  9  3  2  2  8  6  8  1  3  1\n",
      "   3  1  1  5  2  2  1  1  1  2  1  1  4  1  1  1  6  1  1  1  1  1  4  1\n",
      "   1  1  1  1  2  1  5  3  1  1  2  1  1  1  1  1  1  1  3  2  1  1 62  3\n",
      "   9  4  1  1  2  2  1  3  2  1  1  1  1  1  1  2  7  1  1  3  3  2  1  8\n",
      "   5  4  1  1  1  2  1  5  1  1  3  1  1  2  4  3  1  1  1  2  1  2  2  1\n",
      "   3  2  1  1  2  1  1  1  5  1  1  1  1  1  1  4  1  1  2  1  4  5  1  1\n",
      "   4  1  1  1  4  1  1  1  1  3  4  2  4  1  1  5  3  1  1  1  2  2  2  1\n",
      "   2  1  5  1  1  3  5  2  2  3  1  3  4  5  2  1  1  2  3  1  1  1  2  1\n",
      "   1  1  1  1  5  2  1  4  1  1  1  1  6  1  2  1  2  1  1  4  3  3  1  1\n",
      "   1  3  2  2  1  1  2  1  1  1  2  2  2  3  9  4  2  1 21  1  1  2  1  1\n",
      "   1  1  7  1  1  1  3  1  1  1  1  1  1  1  2  2  1  2  1  1  1  2  1  1\n",
      "   2 15  1  1  1  1  1  2  1  3  2  1  2  1  1  2  1  1  1  1  1  1  2  3\n",
      "   1  1  1  1  1  1  2  1  3  3  4  1  3  7  3  2  1  2  1  1  2  7  1  1\n",
      "   1  5  1  1  2  1  1  2  2  1  2  3  2  2  2  1  2  7  1  1  1  1  1  1\n",
      "   1  1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "# Download required NLTK resources\n",
    "'''nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''\n",
    "# Read the content of the file\n",
    "file_path = 'text3.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# Initialize the stopwords, lemmatizer, and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation and stopwords, and perform lemmatization and stemming\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)  # Lemmatization\n",
    "            stemmed_word = stemmer.stem(lemmatized_word)  # Stemming\n",
    "            processed_tokens.append(stemmed_word)\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Preprocess the text data\n",
    "preprocessed_text = preprocess_text(text_data)\n",
    "\n",
    "# Initialize the CountVectorizer (Bag of Words model)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data to create the BoW model\n",
    "X = vectorizer.fit_transform([preprocessed_text])\n",
    "\n",
    "# Extract the vocabulary (words and their corresponding indices)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the BoW model to an array for easy viewing\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Print the vocabulary and its corresponding vector\n",
    "print(\"Vocabulary:\\n\", vocabulary)\n",
    "print(\"\\nBag of Words (Vectorization):\\n\", bow_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da811087-ddf6-45fb-af92-a8b852638723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Vocabulary:\n",
      " ['abil' 'account' 'accur' 'accuraci' 'across' 'action' 'activ' 'addit'\n",
      " 'address' 'adjust' 'advanc' 'age' 'ai' 'alert' 'algorithm' 'allow' 'also'\n",
      " 'amount' 'analysi' 'analyt' 'analyz' 'anomali' 'applic' 'area' 'artifici'\n",
      " 'aspect' 'ass' 'autom' 'automat' 'autonom' 'avail' 'awar' 'bandwidth'\n",
      " 'base' 'becom' 'benefit' 'bia' 'bring' 'brought' 'busi' 'capabl' 'care'\n",
      " 'central' 'chain' 'characterist' 'chart' 'clean' 'clinic' 'closer'\n",
      " 'cloud' 'combin' 'competit' 'complex' 'compon' 'comput' 'concern'\n",
      " 'conclus' 'constitut' 'continu' 'converg' 'core' 'correct' 'cours'\n",
      " 'creat' 'credit' 'critic' 'crucial' 'custom' 'cybersecur' 'dashboard'\n",
      " 'data' 'dataset' 'decis' 'decision' 'deep' 'deeper' 'demand' 'deploy'\n",
      " 'design' 'detect' 'develop' 'devic' 'diagnosi' 'digit' 'discriminatori'\n",
      " 'disrupt' 'divers' 'drive' 'driven' 'dynam' 'earli' 'edg' 'effici'\n",
      " 'effort' 'embrac' 'enabl' 'enhanc' 'ensur' 'entiti' 'error' 'essay'\n",
      " 'essenti' 'establish' 'ethic' 'evalu' 'evolv' 'exampl' 'execut' 'explain'\n",
      " 'explor' 'extract' 'facilit' 'fairness' 'far' 'featur' 'field' 'filter'\n",
      " 'financ' 'financi' 'fluctuat' 'focu' 'forecast' 'form' 'format' 'fraudul'\n",
      " 'full' 'fundament' 'futur' 'gener' 'govern' 'graph' 'graphic' 'guidelin'\n",
      " 'handl' 'har' 'healthcar' 'hidden' 'higher' 'highlight' 'histor' 'human'\n",
      " 'identifi' 'imag' 'immedi' 'impact' 'implement' 'import' 'imposs'\n",
      " 'improv' 'inadvert' 'includ' 'inconsist' 'increasingli' 'industri'\n",
      " 'inform' 'innov' 'insight' 'instanc' 'integr' 'intellig' 'interact'\n",
      " 'introduct' 'intuit' 'invalu' 'inventori' 'invest' 'involv' 'issu' 'key'\n",
      " 'knowledg' 'languag' 'larg' 'latenc' 'lead' 'learn' 'level' 'leverag'\n",
      " 'like' 'local' 'machin' 'mak' 'make' 'manag' 'mani' 'manner' 'manual'\n",
      " 'market' 'massiv' 'meaning' 'medic' 'medium' 'method' 'mimic' 'miss'\n",
      " 'mitig' 'ml' 'model' 'monitor' 'multipl' 'natur' 'navig' 'network'\n",
      " 'neural' 'new' 'nlp' 'note' 'numer' 'object' 'often' 'one' 'oper' 'optim'\n",
      " 'organ' 'outcom' 'outlier' 'paper' 'paramet' 'particularli' 'patient'\n",
      " 'pattern' 'perform' 'perpetu' 'person' 'plan' 'platform' 'portfolio'\n",
      " 'portion' 'potenti' 'pow' 'power' 'predict' 'prepar' 'present' 'primari'\n",
      " 'process' 'profound' 'project' 'provid' 'quick' 'quickli' 'rais' 'reach'\n",
      " 'real' 'recent' 'recognit' 'recommend' 'reduc' 'regress' 'regul'\n",
      " 'relationship' 'relev' 'reli' 'repres' 'represent' 'requir' 'research'\n",
      " 'respect' 'respons' 'retail' 'revolution' 'right' 'risk' 'safeti' 'sale'\n",
      " 'scenario' 'scienc' 'scientist' 'second' 'sector' 'seek' 'sensor'\n",
      " 'sentiment' 'server' 'signific' 'significantli' 'similarli' 'social'\n",
      " 'societ' 'sophist' 'sourc' 'speed' 'spent' 'split' 'stakehold' 'stay'\n",
      " 'step' 'strategi' 'stream' 'structur' 'struggl' 'subfield' 'suitabl'\n",
      " 'summar' 'suppli' 'support' 'synergi' 'system' 'task' 'techniqu'\n",
      " 'technolog' 'text' 'tim' 'time' 'today' 'togeth' 'tool' 'trade' 'tradit'\n",
      " 'train' 'transform' 'transpar' 'treatment' 'tree' 'trend' 'trigger'\n",
      " 'uncov' 'understand' 'unfair' 'unpreced' 'unstructur' 'use' 'user' 'valu'\n",
      " 'valuabl' 'variou' 'vast' 'vehicl' 'video' 'vision' 'visual' 'vital'\n",
      " 'way' 'within' 'without' 'workflow' 'world' 'would' 'year']\n",
      "\n",
      "Bag of Words (Vectorization):\n",
      " [[ 2  1  2  2  1  3  2  1  3  1  7  1 34  1  9  3  2  2  8  6  8  1  3  1\n",
      "   3  1  1  5  2  2  1  1  1  2  1  1  4  1  1  1  6  1  1  1  1  1  4  1\n",
      "   1  1  1  1  2  1  5  3  1  1  2  1  1  1  1  1  1  1  3  2  1  1 62  3\n",
      "   9  4  1  1  2  2  1  3  2  1  1  1  1  1  1  2  7  1  1  3  3  2  1  8\n",
      "   5  4  1  1  1  2  1  5  1  1  3  1  1  2  4  3  1  1  1  2  1  2  2  1\n",
      "   3  2  1  1  2  1  1  1  5  1  1  1  1  1  1  4  1  1  2  1  4  5  1  1\n",
      "   4  1  1  1  4  1  1  1  1  3  4  2  4  1  1  5  3  1  1  1  2  2  2  1\n",
      "   2  1  5  1  1  3  5  2  2  3  1  3  4  5  2  1  1  2  3  1  1  1  2  1\n",
      "   1  1  1  1  5  2  1  4  1  1  1  1  6  1  2  1  2  1  1  4  3  3  1  1\n",
      "   1  3  2  2  1  1  2  1  1  1  2  2  2  3  9  4  2  1 21  1  1  2  1  1\n",
      "   1  1  7  1  1  1  3  1  1  1  1  1  1  1  2  2  1  2  1  1  1  2  1  1\n",
      "   2 15  1  1  1  1  1  2  1  3  2  1  2  1  1  2  1  1  1  1  1  1  2  3\n",
      "   1  1  1  1  1  1  2  1  3  3  4  1  3  7  3  2  1  2  1  1  2  7  1  1\n",
      "   1  5  1  1  2  1  1  2  2  1  2  3  2  2  2  1  2  7  1  1  1  1  1  1\n",
      "   1  1]]\n",
      "\n",
      "TensorFlow Word Index (Vocabulary): {'data': 1, 'ai': 2, 'process': 3, 'scienc': 4, 'algorithm': 5, 'predict': 6, 'decis': 7, 'analysi': 8, 'analyz': 9, 'enabl': 10, 'advanc': 11, 'transform': 12, 'driven': 13, 'real': 14, 'tim': 15, 'visual': 16, 'capabl': 17, 'nlp': 18, 'analyt': 19, 'intellig': 20, 'enhanc': 21, 'gener': 22, 'learn': 23, 'identifi': 24, 'trend': 25, 'autom': 26, 'languag': 27, 'comput': 28, 'make': 29, 'model': 30, 'ethic': 31, 'impact': 32, 'healthcar': 33, 'human': 34, 'extract': 35, 'decision': 36, 'mak': 37, 'inform': 38, 'clean': 39, 'prepar': 40, 'improv': 41, 'techniqu': 42, 'natur': 43, 'optim': 44, 'insight': 45, 'ensur': 46, 'bia': 47, 'artifici': 48, 'lead': 49, 'signific': 50, 'focu': 51, 'power': 52, 'effici': 53, 'particularli': 54, 'machin': 55, 'dataset': 56, 'detect': 57, 'valuabl': 58, 'market': 59, 'crucial': 60, 'time': 61, 'task': 62, 'exampl': 63, 'text': 64, 'reduc': 65, 'allow': 66, 'organ': 67, 'outcom': 68, 'industri': 69, 'like': 70, 'applic': 71, 'stream': 72, 'edg': 73, 'action': 74, 'facilit': 75, 'interact': 76, 'system': 77, 'address': 78, 'concern': 79, 'numer': 80, 'field': 81, 'financ': 82, 'abil': 83, 'drive': 84, 'innov': 85, 'explor': 86, 'highlight': 87, 'key': 88, 'often': 89, 'vast': 90, 'amount': 91, 'today': 92, 'involv': 93, 'accuraci': 94, 'pattern': 95, 'manual': 96, 'patient': 97, 'person': 98, 'fraudul': 99, 'activ': 100, 'invest': 101, 'portion': 102, 'significantli': 103, 'vision': 104, 'automat': 105, 'valu': 106, 'unstructur': 107, 'effort': 108, 'requir': 109, 'level': 110, 'forecast': 111, 'develop': 112, 'accur': 113, 'complex': 114, 'base': 115, 'continu': 116, 'inventori': 117, 'manag': 118, 'custom': 119, 'demand': 120, 'strategi': 121, 'autonom': 122, 'vehicl': 123, 'financi': 124, 'leverag': 125, 'social': 126, 'medium': 127, 'sourc': 128, 'essenti': 129, 'deploy': 130, 'scenario': 131, 'respons': 132, 'monitor': 133, 'understand': 134, 'provid': 135, 'tool': 136, 'also': 137, 'pow': 138, 'present': 139, 'sentiment': 140, 'research': 141, 'support': 142, 'potenti': 143, 'risk': 144, 'train': 145, 'use': 146, 'variou': 147, 'introduct': 148, 'recent': 149, 'year': 150, 'converg': 151, 'revolution': 152, 'technolog': 153, 'mimic': 154, 'knowledg': 155, 'togeth': 156, 'form': 157, 'combin': 158, 'essay': 159, 'area': 160, 'one': 161, 'primari': 162, 'way': 163, 'tradit': 164, 'method': 165, 'struggl': 166, 'handl': 167, \"'s\": 168, 'digit': 169, 'age': 170, 'ml': 171, 'deep': 172, 'massiv': 173, 'unpreced': 174, 'speed': 175, 'instanc': 176, 'larg': 177, 'would': 178, 'imposs': 179, 'earli': 180, 'diagnosi': 181, 'treatment': 182, 'plan': 183, 'step': 184, 'workflow': 185, 'account': 186, 'spent': 187, 'project': 188, 'correct': 189, 'error': 190, 'inconsist': 191, 'miss': 192, 'relev': 193, 'structur': 194, 'format': 195, 'suitabl': 196, 'similarli': 197, 'imag': 198, 'video': 199, 'object': 200, 'meaning': 201, 'featur': 202, 'scientist': 203, 'higher': 204, 'core': 205, 'compon': 206, 'futur': 207, 'sophist': 208, 'relationship': 209, 'within': 210, 'regress': 211, 'tree': 212, 'neural': 213, 'network': 214, 'histor': 215, 'new': 216, 'becom': 217, 'avail': 218, 'retail': 219, 'mani': 220, 'trade': 221, 'cybersecur': 222, 'immedi': 223, 'sensor': 224, 'anomali': 225, 'trigger': 226, 'alert': 227, 'split': 228, 'second': 229, 'safeti': 230, 'navig': 231, 'bring': 232, 'closer': 233, 'latenc': 234, 'bandwidth': 235, 'devic': 236, 'local': 237, 'without': 238, 'reli': 239, 'central': 240, 'cloud': 241, 'server': 242, 'quick': 243, 'critic': 244, 'vital': 245, 'aspect': 246, 'stakehold': 247, 'graphic': 248, 'represent': 249, 'deeper': 250, 'intuit': 251, 'characterist': 252, 'outlier': 253, 'creat': 254, 'dashboard': 255, 'user': 256, 'dynam': 257, 'adjust': 258, 'paramet': 259, 'filter': 260, 'uncov': 261, 'hidden': 262, 'platform': 263, 'busi': 264, 'sale': 265, 'chart': 266, 'graph': 267, 'execut': 268, 'quickli': 269, 'subfield': 270, 'constitut': 271, 'perform': 272, 'entiti': 273, 'recognit': 274, 'summar': 275, 'invalu': 276, 'clinic': 277, 'note': 278, 'paper': 279, 'medic': 280, 'care': 281, 'fundament': 282, 'evalu': 283, 'multipl': 284, 'recommend': 285, 'cours': 286, 'suppli': 287, 'chain': 288, 'fluctuat': 289, 'disrupt': 290, 'sector': 291, 'ass': 292, 'credit': 293, 'portfolio': 294, 'oper': 295, 'brought': 296, 'benefit': 297, 'rais': 298, 'import': 299, 'inadvert': 300, 'perpetu': 301, 'unfair': 302, 'discriminatori': 303, 'issu': 304, 'mitig': 305, 'includ': 306, 'fairness': 307, 'awar': 308, 'divers': 309, 'repres': 310, 'implement': 311, 'transpar': 312, 'explain': 313, 'addit': 314, 'guidelin': 315, 'regul': 316, 'establish': 317, 'govern': 318, 'design': 319, 'manner': 320, 'respect': 321, 'right': 322, 'societ': 323, 'conclus': 324, 'profound': 325, 'far': 326, 'reach': 327, 'evolv': 328, 'integr': 329, 'across': 330, 'embrac': 331, 'synergi': 332, 'seek': 333, 'har': 334, 'full': 335, 'stay': 336, 'competit': 337, 'increasingli': 338, 'world': 339}\n",
      "\n",
      "Bag of Words (One-Hot Encoding) from TensorFlow:\n",
      " [[ 0. 62. 34. 21. 15.  9.  9.  9.  8.  8.  8.  7.  7.  7.  7.  7.  7.  6.\n",
      "   6.  6.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  4.  4.  4.  4.\n",
      "   4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  3.  3.  3.  3.  3.  3.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n",
      "\n",
      "TF-IDF Representation from TensorFlow:\n",
      " [[0.         2.0788741  1.83528126 1.63991273 1.50348498 1.29636301\n",
      "  1.29636301 1.29636301 1.2486061  1.2486061  1.2486061  1.19446378\n",
      "  1.19446378 1.19446378 1.19446378 1.19446378 1.19446378 1.13196106\n",
      "  1.13196106 1.13196106 1.05803603 1.05803603 1.05803603 1.05803603\n",
      "  1.05803603 1.05803603 1.05803603 1.05803603 1.05803603 1.05803603\n",
      "  1.05803603 1.05803603 0.9675591  0.9675591  0.9675591  0.9675591\n",
      "  0.9675591  0.9675591  0.9675591  0.9675591  0.9675591  0.9675591\n",
      "  0.9675591  0.9675591  0.9675591  0.9675591  0.9675591  0.9675591\n",
      "  0.85091406 0.85091406 0.85091406 0.85091406 0.85091406 0.85091406\n",
      "  0.85091406 0.85091406 0.85091406 0.85091406 0.85091406 0.85091406\n",
      "  0.85091406 0.85091406 0.85091406 0.85091406 0.85091406 0.85091406\n",
      "  0.85091406 0.85091406 0.85091406 0.85091406 0.85091406 0.85091406\n",
      "  0.85091406 0.85091406 0.85091406 0.85091406 0.85091406 0.85091406\n",
      "  0.85091406 0.85091406 0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121  0.6865121\n",
      "  0.6865121  0.6865121  0.6865121  0.6865121  0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511 0.40546511 0.40546511\n",
      "  0.40546511 0.40546511 0.40546511 0.40546511]]\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "import string\n",
    "\n",
    "# Download required NLTK resources\n",
    "'''nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''\n",
    "# Read the content of the file\n",
    "file_path = 'text3.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# Initialize the stopwords, lemmatizer, and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess text data (Tokenization, Lemmatization, Stemming, Stopwords Removal)\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation and stopwords, and perform lemmatization and stemming\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)  # Lemmatization\n",
    "            stemmed_word = stemmer.stem(lemmatized_word)  # Stemming\n",
    "            processed_tokens.append(stemmed_word)\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Preprocess the text data\n",
    "preprocessed_text = preprocess_text(text_data)\n",
    "\n",
    "# =======================\n",
    "# BAG OF WORDS (BoW) PART\n",
    "# =======================\n",
    "\n",
    "# Initialize the CountVectorizer (Bag of Words model)\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data to create the BoW model\n",
    "X = vectorizer.fit_transform([preprocessed_text])\n",
    "\n",
    "# Extract the vocabulary (words and their corresponding indices)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the BoW model to an array for easy viewing\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Print the BoW results\n",
    "print(\"Bag of Words (BoW) Vocabulary:\\n\", vocabulary)\n",
    "print(\"\\nBag of Words (Vectorization):\\n\", bow_array)\n",
    "\n",
    "# ============================\n",
    "# TENSORFLOW TF-IDF PART STARTS\n",
    "# ============================\n",
    "\n",
    "# Initialize the TensorFlow Tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# Fit tokenizer on the preprocessed text\n",
    "tokenizer.fit_on_texts([preprocessed_text])\n",
    "\n",
    "# Convert text to sequences of integers (word indices)\n",
    "sequences = tokenizer.texts_to_sequences([preprocessed_text])\n",
    "\n",
    "# Get the word index (vocabulary mapping)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert sequences to one-hot encoded form (Bag of Words equivalent)\n",
    "one_hot_results = tokenizer.texts_to_matrix([preprocessed_text], mode='count')\n",
    "\n",
    "# Convert sequences to TF-IDF form\n",
    "tfidf_results = tokenizer.texts_to_matrix([preprocessed_text], mode='tfidf')\n",
    "\n",
    "# ====================\n",
    "# OUTPUT THE RESULTS\n",
    "# ====================\n",
    "\n",
    "# Print TensorFlow's vocabulary and TF-IDF results\n",
    "print(\"\\nTensorFlow Word Index (Vocabulary):\", word_index)\n",
    "print(\"\\nBag of Words (One-Hot Encoding) from TensorFlow:\\n\", one_hot_results)\n",
    "print(\"\\nTF-IDF Representation from TensorFlow:\\n\", tfidf_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5027e631-a994-4ae8-b76e-490106ed0f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">21,824</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m21,824\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,937</span> (93.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,937\u001b[0m (93.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,937</span> (93.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,937\u001b[0m (93.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 826ms/step - accuracy: 1.0000 - loss: 0.4962\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.3755\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2739\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.1912\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1314\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0946\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0697\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0509\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0381\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0285\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 0.0216\n",
      "\n",
      "Final loss: 0.021618764847517014\n",
      "Final accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "import string\n",
    "\n",
    "# Download required NLTK resources\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Read the content of the file\n",
    "file_path = 'text3.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# Initialize the stopwords, lemmatizer, and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess text data (Tokenization, Lemmatization, Stemming, Stopwords Removal)\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation and stopwords, and perform lemmatization and stemming\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)  # Lemmatization\n",
    "            stemmed_word = stemmer.stem(lemmatized_word)  # Stemming\n",
    "            processed_tokens.append(stemmed_word)\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Preprocess the text data\n",
    "preprocessed_text = preprocess_text(text_data)\n",
    "\n",
    "# ============================\n",
    "# TENSORFLOW TF-IDF PART STARTS\n",
    "# ============================\n",
    "\n",
    "# Initialize the TensorFlow Tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# Fit tokenizer on the preprocessed text\n",
    "tokenizer.fit_on_texts([preprocessed_text])\n",
    "\n",
    "# Convert text to TF-IDF form\n",
    "tfidf_results = tokenizer.texts_to_matrix([preprocessed_text], mode='tfidf')\n",
    "\n",
    "# ============================\n",
    "# DEFINE ANN MODEL\n",
    "# ============================\n",
    "\n",
    "# For demo purposes, we create a mock label (you can replace it with your real labels)\n",
    "labels = np.array([1])  # Assuming binary classification (0 or 1), change based on your data\n",
    "\n",
    "# Define the ANN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(tfidf_results.shape[1],)),  # Input layer (TF-IDF input size)\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # First hidden layer with 64 neurons\n",
    "    tf.keras.layers.Dense(32, activation='relu'),  # Second hidden layer with 32 neurons\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model on the TF-IDF data\n",
    "history = model.fit(tfidf_results, labels, epochs=10, verbose=1)\n",
    "\n",
    "# ====================\n",
    "# OUTPUT OF ANN TRAINING\n",
    "# ====================\n",
    "# Evaluate model performance\n",
    "loss, accuracy = model.evaluate(tfidf_results, labels, verbose=1)\n",
    "print(f\"\\nFinal loss: {loss}\")\n",
    "print(f\"Final accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bdb47b-33b3-453c-994f-a7f49c0c21f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
