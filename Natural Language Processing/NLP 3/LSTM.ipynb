{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f37367-910d-4abc-9448-3f80ebc0c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: keras in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4bb095-8a80-4c94-9d96-b2c32a953bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 2.9326\n",
      "Epoch 2/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 2.2379\n",
      "Epoch 3/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.9985\n",
      "Epoch 4/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.8285\n",
      "Epoch 5/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.6803\n",
      "Epoch 6/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.5555\n",
      "Epoch 7/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.4393\n",
      "Epoch 8/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.3217\n",
      "Epoch 9/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - loss: 1.2309\n",
      "Epoch 10/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.1432\n",
      "Epoch 11/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.0568\n",
      "Epoch 12/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - loss: 0.9911\n",
      "Epoch 13/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - loss: 0.9336\n",
      "Epoch 14/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - loss: 0.8703\n",
      "Epoch 15/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - loss: 0.8482\n",
      "Epoch 16/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 79ms/step - loss: 0.7805\n",
      "Epoch 17/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 79ms/step - loss: 0.7586\n",
      "Epoch 18/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 78ms/step - loss: 0.7504\n",
      "Epoch 19/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 78ms/step - loss: 0.7082\n",
      "Epoch 20/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 79ms/step - loss: 0.6857\n",
      "all veziers-le-rethel had followed the funeral procession of m. badon- leremince to the grave, and the the frost might of her stold that did not care for her parents me one's the fireplace.\n",
      "\n",
      "and they have not from my chair to be was a strander them an example, never had been placed in my heart to time. they have not never had said my heart was suddenly a crime, ready to come out. if some time, and the selod and the stars that i suffered in the child cry-chair before they have on from my chair to be was a strander them an example, never had been placed in my heart to time. they have not never h\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'story.txt'  # Replace with your .txt file path\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, 1):\n",
    "    sequences.append(text[i:i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences to integer format\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(seed, length):\n",
    "    generated = seed\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed):\n",
    "            x_pred[0, t, char_to_index[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# Generate new text\n",
    "seed_text = text[:sequence_length]  # Starting with the first sequence\n",
    "generated_text = generate_text(seed_text, 500)  # Generate 500 characters\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08665e0a-2db2-49ea-8311-3094aaaea017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - loss: 3.0619\n",
      "Epoch 2/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 69ms/step - loss: 2.6110\n",
      "Epoch 3/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 2.2423\n",
      "Epoch 4/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 2.0018\n",
      "Epoch 5/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - loss: 1.7758\n",
      "Epoch 6/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 65ms/step - loss: 1.5593\n",
      "Epoch 7/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 1.3996\n",
      "Epoch 8/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 1.2476\n",
      "Epoch 9/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 1.1190\n",
      "Epoch 10/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.9928\n",
      "Epoch 11/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 66ms/step - loss: 0.9214\n",
      "Epoch 12/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - loss: 0.8305\n",
      "Epoch 13/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.7180\n",
      "Epoch 14/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.7005\n",
      "Epoch 15/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.6285\n",
      "Epoch 16/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.5890\n",
      "Epoch 17/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.5114\n",
      "Epoch 18/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.4845\n",
      "Epoch 19/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.4621\n",
      "Epoch 20/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.4143\n",
      "the impact of artificial intelligence on data science\n",
      "introduction\n",
      "in recent years, the convergence of and outlial for example, ai-powered nle sensure analysis, entity to ensure manual essential for making transformed data processing capabilities, accowore morket transformed data processing capabilities, accowore morket transformed data processing capabilities, accowore morket transformed data processing capabilities, accowore morket transformed data processing capabilities, accowore morket transformed data processing capabilities, accowore morket transformed data processing capabilities, acco\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'text3.txt'  # Replace with your .txt file path\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, 1):\n",
    "    sequences.append(text[i:i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences to integer format\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(seed, length):\n",
    "    generated = seed\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed):\n",
    "            x_pred[0, t, char_to_index[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# Generate new text\n",
    "seed_text = text[:sequence_length]  # Starting with the first sequence\n",
    "generated_text = generate_text(seed_text, 500)  # Generate 500 characters\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3417c46f-a775-4906-9df7-b4e42174499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step - loss: 2.9458\n",
      "Epoch 2/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 2.2906\n",
      "Epoch 3/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 2.0541\n",
      "Epoch 4/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.8668\n",
      "Epoch 5/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.7507\n",
      "Epoch 6/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.6214\n",
      "Epoch 7/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.5074\n",
      "Epoch 8/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.3756\n",
      "Epoch 9/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.2885\n",
      "Epoch 10/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.1763\n",
      "Epoch 11/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 1.1092\n",
      "Epoch 12/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.0609\n",
      "Epoch 13/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.9783\n",
      "Epoch 14/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 0.9056\n",
      "Epoch 15/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - loss: 0.8710\n",
      "Epoch 16/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.8234\n",
      "Epoch 17/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.8114\n",
      "Epoch 18/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.7557\n",
      "Epoch 19/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.7315\n",
      "Epoch 20/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.6878\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a seed text:  My children, my dear children, I could not sleep the eternal sleep in peace if I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "                     my children, my dear children, i could not sleep the eternal sleep in peace if i was an hour, and saided the window the strokes mothered them they were presently deslighted her parents' consent the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child coughed again, and the child\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'story.txt'  # Replace with your .txt file path\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, 1):\n",
    "    sequences.append(text[i:i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences to integer format\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to generate text based on user input\n",
    "def generate_text_based_on_input(seed, length=500):\n",
    "    generated = seed\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed[-sequence_length:]):\n",
    "            if char in char_to_index:\n",
    "                x_pred[0, t, char_to_index[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# Get user input and generate text\n",
    "user_input = input(\"Enter a seed text: \").lower()  # Convert input to lowercase for consistency\n",
    "if len(user_input) < sequence_length:\n",
    "    # If the input is too short, pad it with spaces\n",
    "    user_input = ' ' * (sequence_length - len(user_input)) + user_input\n",
    "\n",
    "generated_text = generate_text_based_on_input(user_input, 500)  # Generate 500 characters\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251b2fc3-5b54-48b0-a22c-caf834ebc71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - loss: 2.9647\n",
      "Epoch 2/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - loss: 2.2791\n",
      "Epoch 3/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 2.0423\n",
      "Epoch 4/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.8638\n",
      "Epoch 5/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 1.7517\n",
      "Epoch 6/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 1.5960\n",
      "Epoch 7/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 1.4962\n",
      "Epoch 8/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 1.3756\n",
      "Epoch 9/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 1.2786\n",
      "Epoch 10/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.1815\n",
      "Epoch 11/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 1.0831\n",
      "Epoch 12/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.0043\n",
      "Epoch 13/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.9850\n",
      "Epoch 14/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 0.8991\n",
      "Epoch 15/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 0.8368\n",
      "Epoch 16/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 0.8337\n",
      "Epoch 17/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 62ms/step - loss: 0.7777\n",
      "Epoch 18/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.7415\n",
      "Epoch 19/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 0.7063\n",
      "Epoch 20/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.6638\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a seed text:  All Veziers-le-Rethel had followed the funeral procession of M. Badon-Leremince to the grave, and the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " all veziers-le-rethel had followed the funeral procession of m. badon-leremince to the grave, and the window the destrosen an the end of a woman being the window the frightfully his spectacles with that i seemed to the end of a woman being the window the frightfully his spectacles with that i seemed to the end of a woman being the window the frightfully his spectacles with that i seemed to the end of a woman being the window the frightfully his spectacles with that i seemed to the end of a woman being the window the frightfully his spectacles with that i seemed to the end of a woman being the w\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'story.txt'  # Replace with your .txt file path\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, 1):\n",
    "    sequences.append(text[i:i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences to integer format\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to generate text based on user input\n",
    "def generate_text_based_on_input(seed, length=500):\n",
    "    generated = seed\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed[-sequence_length:]):\n",
    "            if char in char_to_index:\n",
    "                x_pred[0, t, char_to_index[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# Get user input and generate text\n",
    "user_input = input(\"Enter a seed text: \").lower()  # Convert input to lowercase for consistency\n",
    "if len(user_input) < sequence_length:\n",
    "    # If the input is too short, pad it with spaces\n",
    "    user_input = ' ' * (sequence_length - len(user_input)) + user_input\n",
    "\n",
    "generated_text = generate_text_based_on_input(user_input, 500)  # Generate 500 characters\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e606230a-8322-4f7d-a1b0-953d576afd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - loss: 2.9391\n",
      "Epoch 2/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 2.2775\n",
      "Epoch 3/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step - loss: 2.0429\n",
      "Epoch 4/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.8611\n",
      "Epoch 5/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - loss: 1.7257\n",
      "Epoch 6/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.6239\n",
      "Epoch 7/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.4916\n",
      "Epoch 8/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.3634\n",
      "Epoch 9/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 76ms/step - loss: 1.2802\n",
      "Epoch 10/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - loss: 1.1956\n",
      "Epoch 11/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 1.0818\n",
      "Epoch 12/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - loss: 1.0210\n",
      "Epoch 13/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 0.9462\n",
      "Epoch 14/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.9086\n",
      "Epoch 15/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.8537\n",
      "Epoch 16/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 0.7936\n",
      "Epoch 17/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.7640\n",
      "Epoch 18/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 0.7019\n",
      "Epoch 19/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.6817\n",
      "Epoch 20/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.6417\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a seed text to start generating text:  All Veziers-le-Rethel had followed the funeral procession of M. Badon-Leremince to the grave, and the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      " all veziers-le-rethel had followed the funeral procession of m. badon-leremince to the grave, and the took the resartice to the windowan tound, in me or? he evering. the with her a brist for the suffering only are dang--silentus?\"\n",
      "\n",
      "wher it awleep if a men the tall--the likenaniso it mp, and je? he last become becoming flech.\n",
      "\n",
      "but more fremed at the fetter when a reast recolacked by to mad in beside my befunte from my wind; and i madt when i was enseasles with the evening. i tervouset of mournct, and shutis alone in the windration who was take.\n",
      "\n",
      "he wind the fetter of a gander untow at horblack.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'story.txt'  # Path to your story.txt\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, 1):\n",
    "    sequences.append(text[i:i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences to integer format\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to apply temperature sampling\n",
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-10) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function to generate text based on user input with temperature\n",
    "def generate_text_based_on_input(seed, length=500, temperature=1.0):\n",
    "    generated = seed\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed[-sequence_length:]):\n",
    "            if char in char_to_index:\n",
    "                x_pred[0, t, char_to_index[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample_with_temperature(preds, temperature)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Enter a seed text to start generating text: \").lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Ensure the input is at least 100 characters long by padding if needed\n",
    "if len(user_input) < sequence_length:\n",
    "    user_input = ' ' * (sequence_length - len(user_input)) + user_input\n",
    "\n",
    "# Generate new text based on the user's input\n",
    "temperature = 0.8  # Adjust this value between 0.5 (more predictable) to 1.5 (more random)\n",
    "generated_text = generate_text_based_on_input(user_input, 500, temperature=temperature)  # Generate 500 characters\n",
    "\n",
    "print(\"\\nGenerated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f0b201-9ff1-4c4d-b510-73b1952a4b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 63ms/step - loss: 2.9254\n",
      "Epoch 2/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 2.2345\n",
      "Epoch 3/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 2.0033\n",
      "Epoch 4/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 1.8478\n",
      "Epoch 5/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 1.7384\n",
      "Epoch 6/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - loss: 1.6068\n",
      "Epoch 7/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - loss: 1.4370\n",
      "Epoch 8/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - loss: 1.3639\n",
      "Epoch 9/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.2820\n",
      "Epoch 10/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.1884\n",
      "Epoch 11/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.0942\n",
      "Epoch 12/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 1.0235\n",
      "Epoch 13/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.9511\n",
      "Epoch 14/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.9130\n",
      "Epoch 15/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 0.8794\n",
      "Epoch 16/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.7815\n",
      "Epoch 17/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.7733\n",
      "Epoch 18/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - loss: 0.7389\n",
      "Epoch 19/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - loss: 0.6914\n",
      "Epoch 20/20\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 0.6546\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a seed text to start generating text:  every step he took, is every step closer to danger, so he said his name was peter heisenberg and asked jesse pinkman to join in in his underworld job to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      " every step he took, is every step closer to danger, so he said his name was peter heisenberg and asked jesse pinkman to join in in his underworld job to me, and i had dines spaw the stars was a that i see terrible was tounderal as in the confession of mondings the envelope.\n",
      "\n",
      "m. poirel de little coughs of suppace, asoral not know bean society of the surfucs of the disuracidesiling to such of his one surfession of tongs who cannot ence, who beot her parents light!\n",
      "\n",
      "he dat daughtar. i cannot express mell the crib.\n",
      "\n",
      "hew was mother lay containing to one's sillel, gone that i delt terrible calsived, and ime aw a youcted her peppanion, this was sleeps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'story.txt'  # Path to your story.txt\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, 1):\n",
    "    sequences.append(text[i:i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences to integer format\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to apply temperature sampling\n",
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-10) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function to generate text based on user input with temperature\n",
    "def generate_text_based_on_input(seed, length=500, temperature=1.0):\n",
    "    generated = seed\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed[-sequence_length:]):\n",
    "            if char in char_to_index:\n",
    "                x_pred[0, t, char_to_index[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample_with_temperature(preds, temperature)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Enter a seed text to start generating text: \").lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Ensure the input is at least 100 characters long by padding if needed\n",
    "if len(user_input) < sequence_length:\n",
    "    user_input = ' ' * (sequence_length - len(user_input)) + user_input\n",
    "\n",
    "# Generate new text based on the user's input\n",
    "temperature = 0.8  # Adjust this value between 0.5 (more predictable) to 1.5 (more random)\n",
    "generated_text = generate_text_based_on_input(user_input, 500, temperature=temperature)  # Generate 500 characters\n",
    "\n",
    "print(\"\\nGenerated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a5ca73-b0c1-4203-8784-29e66ac28d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 62ms/step - loss: 3.0079\n",
      "Epoch 2/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 2.4113\n",
      "Epoch 3/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 2.0721\n",
      "Epoch 4/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 1.7636\n",
      "Epoch 5/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 1.5522\n",
      "Epoch 6/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 1.3755\n",
      "Epoch 7/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 1.1908\n",
      "Epoch 8/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 1.0528\n",
      "Epoch 9/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 0.9187\n",
      "Epoch 10/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.8101\n",
      "Epoch 11/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.7696\n",
      "Epoch 12/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.6564\n",
      "Epoch 13/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.5807\n",
      "Epoch 14/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.5456\n",
      "Epoch 15/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.4916\n",
      "Epoch 16/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.4330\n",
      "Epoch 17/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.4294\n",
      "Epoch 18/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.3755\n",
      "Epoch 19/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.3558\n",
      "Epoch 20/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 64ms/step - loss: 0.3320\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a seed text to start generating text:  AI-driven data visualization tools can automatically\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "                                                 ai-driven data visualization tools can automatically generataly important that outcomes based on hast ai has tranigation.\n",
      "\n",
      "edge comons, and tagres, identify patterns. addations can the impact of ai in lea algorithms can prodising sociotes such as autofomation. techniques like straage to lights and trends intigting as, edeabled pledit or ai und sterat data science, which traising of the riskons, and preparation, advanced data visualizations based on the interactive databal complex risysns in the crakn souruted toolds and execument ai has suprers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'text3.txt'  # Path to your story.txt\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, 1):\n",
    "    sequences.append(text[i:i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences to integer format\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to apply temperature sampling\n",
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-10) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function to generate text based on user input with temperature\n",
    "def generate_text_based_on_input(seed, length=500, temperature=1.0):\n",
    "    generated = seed\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed[-sequence_length:]):\n",
    "            if char in char_to_index:\n",
    "                x_pred[0, t, char_to_index[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample_with_temperature(preds, temperature)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Enter a seed text to start generating text: \").lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Ensure the input is at least 100 characters long by padding if needed\n",
    "if len(user_input) < sequence_length:\n",
    "    user_input = ' ' * (sequence_length - len(user_input)) + user_input\n",
    "\n",
    "# Generate new text based on the user's input\n",
    "temperature = 0.8  # Adjust this value between 0.5 (more predictable) to 1.5 (more random)\n",
    "generated_text = generate_text_based_on_input(user_input, 500, temperature=temperature)  # Generate 500 characters\n",
    "\n",
    "print(\"\\nGenerated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f82657-ff95-41b4-a794-186864e4037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 63ms/step - loss: 3.0312\n",
      "Epoch 2/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - loss: 2.4389\n",
      "Epoch 3/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 2.0864\n",
      "Epoch 4/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 1.8216\n",
      "Epoch 5/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 1.5832\n",
      "Epoch 6/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 1.3941\n",
      "Epoch 7/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 1.2256\n",
      "Epoch 8/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 1.0681\n",
      "Epoch 9/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 0.9520\n",
      "Epoch 10/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.8519\n",
      "Epoch 11/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - loss: 0.7699\n",
      "Epoch 12/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.6759\n",
      "Epoch 13/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 0.6146\n",
      "Epoch 14/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.5593\n",
      "Epoch 15/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 0.5197\n",
      "Epoch 16/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.4633\n",
      "Epoch 17/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - loss: 0.4710\n",
      "Epoch 18/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.3977\n",
      "Epoch 19/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.3992\n",
      "Epoch 20/20\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step - loss: 0.3407\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a seed text to start generating text:  improve and understand\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "                                                                               improve and understand a dredve spent on action.\n",
      "\n",
      "crighes in al this. the vast improved decisions within techniques in a surtiquer innovation\n",
      "these tomly valuarly this can ai husting recolvential for making spess, aid techniquss bisting data science, which focuses on healthes and preparations. ai has transformed natural languate processing tevinion techniques can process can analyze machine learning and moces and trinssing processing inducting reclarization\n",
      "tatked techniques tht requiremention\n",
      "data visualization\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'text3.txt'  # Path to your story.txt\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - sequence_length, 1):\n",
    "    sequences.append(text[i:i + sequence_length])\n",
    "    next_chars.append(text[i + sequence_length])\n",
    "\n",
    "# Convert sequences to integer format\n",
    "X = np.zeros((len(sequences), sequence_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char_to_index[char]] = 1\n",
    "    y[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(sequence_length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=128, epochs=20)\n",
    "\n",
    "# Function to apply temperature sampling\n",
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds + 1e-10) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function to generate text based on user input with temperature\n",
    "def generate_text_based_on_input(seed, length=500, temperature=1.0):\n",
    "    generated = seed\n",
    "    for _ in range(length):\n",
    "        x_pred = np.zeros((1, sequence_length, len(chars)))\n",
    "        for t, char in enumerate(seed[-sequence_length:]):\n",
    "            if char in char_to_index:\n",
    "                x_pred[0, t, char_to_index[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample_with_temperature(preds, temperature)\n",
    "        next_char = index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        seed = seed[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"Enter a seed text to start generating text: \").lower()  # Convert to lowercase for consistency\n",
    "\n",
    "# Ensure the input is at least 100 characters long by padding if needed\n",
    "if len(user_input) < sequence_length:\n",
    "    user_input = ' ' * (sequence_length - len(user_input)) + user_input\n",
    "\n",
    "# Generate new text based on the user's input\n",
    "temperature = 0.8  # Adjust this value between 0.5 (more predictable) to 1.5 (more random)\n",
    "generated_text = generate_text_based_on_input(user_input, 500, temperature=temperature)  # Generate 500 characters\n",
    "\n",
    "print(\"\\nGenerated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8bddc-ed76-47c2-8c42-10193d55ac03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
