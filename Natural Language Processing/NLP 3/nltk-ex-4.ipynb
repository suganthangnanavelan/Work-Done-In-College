{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f68e97-7003-4655-a9e5-e12a5bc85307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['line', 'guitar', 'jazz', 'jazz']\n",
      "The predicted class for the test document is: g\n",
      "Most Informative Features\n",
      "                    line = True                g : f      =      2.0 : 1.0\n",
      "                  smoked = None                g : f      =      2.0 : 1.0\n",
      "                    haul = None                g : f      =      1.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "# Training data\n",
    "train_data = [\n",
    "    (dict([(word, True) for word in word_tokenize('fish smoked fish')]), 'f'),\n",
    "    (dict([(word, True) for word in word_tokenize('fish line')]), 'f'),\n",
    "    (dict([(word, True) for word in word_tokenize('fish haul smoked')]), 'f'),\n",
    "    (dict([(word, True) for word in word_tokenize('guitar jazz line')]), 'g')\n",
    "]\n",
    "\n",
    "# Train the classifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Test data\n",
    "test_data = word_tokenize('line guitar jazz jazz')\n",
    "test_features = dict([(word, True) for word in test_data])\n",
    "\n",
    "# Predict the class for the test document\n",
    "predicted_class = classifier.classify(test_features)\n",
    "print(test_data)\n",
    "print(f'The predicted class for the test document is: {predicted_class}')\n",
    "\n",
    "# Show the most informative features\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d249a4f2-f6d1-453d-b945-c4928df0943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(d5|f) ≈ 2.9749509133099296e-05\n",
      "P(d5|g) ≈ 0.0005225684238029916\n",
      "The predicted class for the test document is: g\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Vocabulary (V)\n",
    "vocab = ['fish', 'smoked', 'line', 'haul', 'guitar', 'jazz']\n",
    "V = len(vocab)\n",
    "\n",
    "# Training data (using word counts)\n",
    "train_data = {\n",
    "    'f': ['fish smoked fish', 'fish line', 'fish haul smoked'],\n",
    "    'g': ['guitar jazz line'],\n",
    "    'h': ['line guitar jazz']\n",
    "}\n",
    "\n",
    "# Priors\n",
    "N = sum(len(doc.split()) for docs in train_data.values() for doc in docs)  # Total number of words\n",
    "N_f = sum(len(doc.split()) for doc in train_data['f'])  # Number of words in class 'f'\n",
    "N_g = sum(len(doc.split()) for doc in train_data['g'])  # Number of words in class 'g'\n",
    "\n",
    "P_f = N_f / N  # Prior for class 'f'\n",
    "P_g = N_g / N  # Prior for class 'g'\n",
    "\n",
    "# Function to calculate conditional probabilities\n",
    "def conditional_probability(word, class_docs, total_class_words):\n",
    "    word_count = sum(doc.split().count(word) for doc in class_docs)\n",
    "    return (word_count + 1) / (total_class_words + V)  # Additive smoothing\n",
    "\n",
    "# Conditional probabilities for class 'f'\n",
    "P_line_f = conditional_probability('line', train_data['f'], N_f)\n",
    "P_guitar_f = conditional_probability('guitar', train_data['f'], N_f)\n",
    "P_jazz_f = conditional_probability('jazz', train_data['f'], N_f)\n",
    "\n",
    "# Conditional probabilities for class 'g'\n",
    "P_line_g = conditional_probability('line', train_data['g'], N_g)\n",
    "P_guitar_g = conditional_probability('guitar', train_data['g'], N_g)\n",
    "P_jazz_g = conditional_probability('jazz', train_data['g'], N_g)\n",
    "\n",
    "# Test document\n",
    "test_doc = 'line guitar jazz jazz'.split()\n",
    "\n",
    "# Calculate the probabilities for each class\n",
    "P_d5_f = P_f * P_line_f * P_guitar_f * P_jazz_f * P_jazz_f\n",
    "P_d5_g = P_g * P_line_g * P_guitar_g * P_jazz_g * P_jazz_g\n",
    "\n",
    "# Choosing the class with the highest probability\n",
    "if P_d5_f > P_d5_g:\n",
    "    predicted_class = 'f'\n",
    "else:\n",
    "    predicted_class = 'g'\n",
    "\n",
    "# Output results\n",
    "print(f\"P(d5|f) ≈ {P_d5_f}\")\n",
    "print(f\"P(d5|g) ≈ {P_d5_g}\")\n",
    "print(f\"The predicted class for the test document is: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa29b3e-88b5-4921-a3d6-fc7ae1f9d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for 'The latest update includes new features for security.': sports\n",
      "\n",
      "Class: sports\n",
      "  P(the|sports) = 0.25000\n",
      "  P(latest|sports) (Laplace smoothed) = 0.10000\n",
      "  P(update|sports) (Laplace smoothed) = 0.10000\n",
      "  P(includes|sports) (Laplace smoothed) = 0.10000\n",
      "  P(new|sports) (Laplace smoothed) = 0.10000\n",
      "  P(features|sports) (Laplace smoothed) = 0.10000\n",
      "  P(for|sports) (Laplace smoothed) = 0.10000\n",
      "  P(security|sports) (Laplace smoothed) = 0.10000\n",
      "  P(.|sports) = 0.12500\n",
      "  Log-probability: -20.68244\n",
      "\n",
      "Class: politics\n",
      "  P(the|politics) = 0.13333\n",
      "  P(latest|politics) (Laplace smoothed) = 0.10000\n",
      "  P(update|politics) (Laplace smoothed) = 0.10000\n",
      "  P(includes|politics) (Laplace smoothed) = 0.10000\n",
      "  P(new|politics) = 0.06667\n",
      "  P(features|politics) (Laplace smoothed) = 0.10000\n",
      "  P(for|politics) (Laplace smoothed) = 0.10000\n",
      "  P(security|politics) (Laplace smoothed) = 0.10000\n",
      "  P(.|politics) = 0.13333\n",
      "  Log-probability: -21.65198\n",
      "\n",
      "Class: technology\n",
      "  P(the|technology) = 0.14286\n",
      "  P(latest|technology) = 0.07143\n",
      "  P(update|technology) = 0.07143\n",
      "  P(includes|technology) (Laplace smoothed) = 0.10000\n",
      "  P(new|technology) = 0.07143\n",
      "  P(features|technology) = 0.07143\n",
      "  P(for|technology) (Laplace smoothed) = 0.10000\n",
      "  P(security|technology) = 0.07143\n",
      "  P(.|technology) = 0.14286\n",
      "  Log-probability: -22.79089\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Sample training data (documents and their respective classes)\n",
    "training_data = [\n",
    "    (\"The team won the football match.\", \"sports\"),\n",
    "    (\"The government passed a new law.\", \"politics\"),\n",
    "    (\"The latest smartphone has great features.\", \"technology\"),\n",
    "    (\"The player scored a goal in the match.\", \"sports\"),\n",
    "    (\"The senator gave a speech on healthcare.\", \"politics\"),\n",
    "    (\"The new software update improves security.\", \"technology\")\n",
    "]\n",
    "\n",
    "# Feature extraction function (tokenizing words)\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    return words\n",
    "\n",
    "# Naive Bayes Document Classifier\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, training_data):\n",
    "        self.feature_prob = defaultdict(lambda: defaultdict(float))\n",
    "        self.class_prob = defaultdict(float)\n",
    "        self.train(training_data)\n",
    "\n",
    "    def train(self, training_data):\n",
    "        # Calculate prior probabilities for classes\n",
    "        class_counts = Counter()\n",
    "        feature_counts = defaultdict(Counter)\n",
    "\n",
    "        for document, category in training_data:\n",
    "            features = extract_features(document)\n",
    "            class_counts[category] += 1\n",
    "            for feature in features:\n",
    "                feature_counts[category][feature] += 1\n",
    "\n",
    "        total_documents = sum(class_counts.values())\n",
    "        self.class_prob = {category: count / total_documents for category, count in class_counts.items()}\n",
    "\n",
    "        # Calculate feature probabilities P(feature|category)\n",
    "        for category, features in feature_counts.items():\n",
    "            total_features = sum(features.values())\n",
    "            self.feature_prob[category] = {feature: (count / total_features) for feature, count in features.items()}\n",
    "\n",
    "    def classify(self, document):\n",
    "        features = extract_features(document)\n",
    "        max_prob = float('-inf')\n",
    "        best_class = None\n",
    "\n",
    "        for category in self.class_prob:\n",
    "            log_prob = math.log(self.class_prob[category])\n",
    "            for feature in features:\n",
    "                if feature in self.feature_prob[category]:\n",
    "                    log_prob += math.log(self.feature_prob[category][feature])\n",
    "                else:\n",
    "                    # Apply Laplace smoothing for unseen features\n",
    "                    log_prob += math.log(1 / (sum(self.feature_prob[category].values()) + len(features)))\n",
    "\n",
    "            if log_prob > max_prob:\n",
    "                max_prob = log_prob\n",
    "                best_class = category\n",
    "\n",
    "        return best_class\n",
    "\n",
    "# Train the model\n",
    "nb_classifier = NaiveBayesClassifier(training_data)\n",
    "\n",
    "# Test the model with a new document\n",
    "test_document = \"The latest update includes new features for security.\"\n",
    "predicted_class = nb_classifier.classify(test_document)\n",
    "print(f\"Predicted class for '{test_document}': {predicted_class}\")\n",
    "\n",
    "# Calculate and display the conditional probabilities for comparison\n",
    "features = extract_features(test_document)\n",
    "for category in nb_classifier.class_prob:\n",
    "    log_prob = math.log(nb_classifier.class_prob[category])\n",
    "    print(f\"\\nClass: {category}\")\n",
    "    for feature in features:\n",
    "        if feature in nb_classifier.feature_prob[category]:\n",
    "            prob = nb_classifier.feature_prob[category][feature]\n",
    "            log_prob += math.log(prob)\n",
    "            print(f\"  P({feature}|{category}) = {prob:.5f}\")\n",
    "        else:\n",
    "            prob = 1 / (sum(nb_classifier.feature_prob[category].values()) + len(features))\n",
    "            log_prob += math.log(prob)\n",
    "            print(f\"  P({feature}|{category}) (Laplace smoothed) = {prob:.5f}\")\n",
    "    print(f\"  Log-probability: {log_prob:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "091c2afd-3fa3-4fc5-b811-7cab42058f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: defaultdict(<class 'float'>, {'fish': -8.44644878689048, 'instrument': -8.160518247477505, 'music': -7.491087593534877})\n",
      "Predicted Class for target words ['Bass', 'haul', 'line']: music\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Given data\n",
    "data = {\n",
    "    1: ['Bass', 'eat', 'super'],\n",
    "    2: ['Bass', 'lunch', 'excellent'],\n",
    "    3: ['Bass', 'ate', 'like'],\n",
    "    4: ['guitar', 'play', 'music'],\n",
    "    5: ['Bass', 'interest', 'pay','line'],\n",
    "    6: ['guitar','play','melody'],\n",
    "    7: ['fish', 'haul','line']\n",
    "}\n",
    "\n",
    "# Corresponding classes (senses)\n",
    "classes = {\n",
    "    1: 'fish',\n",
    "    2: 'fish',\n",
    "    3: 'fish',\n",
    "    4: 'instrument',\n",
    "    5: 'music',\n",
    "    6: 'instrument',\n",
    "    7: 'fish'\n",
    "}\n",
    "\n",
    "# 1) Calculate priors\n",
    "class_counts = defaultdict(int)\n",
    "for cls in classes.values():\n",
    "    class_counts[cls] += 1\n",
    "\n",
    "# Number of documents\n",
    "N = len(classes)\n",
    "\n",
    "# Calculate prior probabilities\n",
    "priors = {cls: count / N for cls, count in class_counts.items()}\n",
    "\n",
    "# 2) Calculate the conditional probability of each word with each class\n",
    "word_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for idx, words in data.items():\n",
    "    cls = classes[idx]\n",
    "    for word in words:\n",
    "        word_counts[cls][word] += 1\n",
    "\n",
    "# Total words per class\n",
    "total_words_per_class = {cls: sum(counts.values()) for cls, counts in word_counts.items()}\n",
    "\n",
    "# Calculate conditional probabilities\n",
    "conditional_probabilities = defaultdict(dict)\n",
    "\n",
    "for cls, words in word_counts.items():\n",
    "    for word, count in words.items():\n",
    "        # Use Laplace smoothing\n",
    "        conditional_probabilities[cls][word] = (count + 1) / (total_words_per_class[cls] + len(word_counts[cls]))\n",
    "\n",
    "# 3) Define the target words and find v (count of words in to-be-found case/test case)\n",
    "target_words = ['Bass', 'haul', 'line']\n",
    "\n",
    "# 4) Score calculation\n",
    "scores = defaultdict(float)\n",
    "\n",
    "# Calculate scores for each class\n",
    "for cls in priors.keys():\n",
    "    scores[cls] = math.log(priors[cls])  # Initialize with log prior\n",
    "\n",
    "    for word in target_words:\n",
    "        vj = word.lower()  # Convert word to lower case for comparison\n",
    "        if vj in conditional_probabilities[cls]:\n",
    "            scores[cls] += math.log(conditional_probabilities[cls][vj])\n",
    "        else:\n",
    "            # If the word is not found, assume a small probability (Laplace smoothing)\n",
    "            scores[cls] += math.log(1 / (total_words_per_class[cls] + len(word_counts[cls])))\n",
    "\n",
    "# Determine the class with the highest score\n",
    "predicted_class = max(scores, key=scores.get)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Scores: {scores}\")\n",
    "print(f\"Predicted Class for target words {target_words}: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9f2051b-ea3a-4817-b48b-2357cc41cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ENTER target words: guitar bass melody interest pay rate play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: defaultdict(<class 'float'>, {'fish': -23.233017526731523, 'instrument': -20.29826933979052, 'finance': -21.373095366600488})\n",
      "Predicted Class for target words ['guitar', 'bass', 'melody', 'interest', 'pay', 'rate', 'play']: instrument\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Given data\n",
    "data = {\n",
    "    1: ['bass', 'eat', 'amount'],\n",
    "    2: ['bass', 'lunch', 'excellent'],\n",
    "    3: ['bass', 'ate','like'],\n",
    "    4: ['guitar', 'play', 'music'],\n",
    "    5: ['money', 'interest', 'pay','amount'],\n",
    "    6: ['guitar','interest','melody'],\n",
    "    7: ['fish', 'haul','line'],\n",
    "    8: ['guitar','like','play'],\n",
    "    9: ['rate']\n",
    "}\n",
    "\n",
    "# Corresponding classes (senses)\n",
    "classes = {\n",
    "    1: 'fish',\n",
    "    2: 'fish',\n",
    "    3: 'fish',\n",
    "    4: 'instrument',\n",
    "    5: 'finance',\n",
    "    6: 'instrument',\n",
    "    7: 'fish',\n",
    "    8: 'instrument',\n",
    "    9: 'finance'\n",
    "}\n",
    "\n",
    "# 1) Calculate priors\n",
    "class_counts = defaultdict(int)\n",
    "for cls in classes.values():\n",
    "    class_counts[cls] += 1\n",
    "\n",
    "# Number of documents\n",
    "N = len(classes)\n",
    "\n",
    "# Calculate prior probabilities\n",
    "priors = {cls: count / N for cls, count in class_counts.items()}\n",
    "\n",
    "# 2) Calculate the conditional probability of each word with each class\n",
    "word_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for idx, words in data.items():\n",
    "    cls = classes[idx]\n",
    "    for word in words:\n",
    "        word_counts[cls][word.lower()] += 1  # Convert words to lowercase for consistency\n",
    "\n",
    "# Total words per class\n",
    "total_words_per_class = {cls: sum(counts.values()) for cls, counts in word_counts.items()}\n",
    "\n",
    "# Full vocabulary size\n",
    "vocab = set(word.lower() for words in data.values() for word in words)\n",
    "V = len(vocab)\n",
    "\n",
    "# Calculate conditional probabilities\n",
    "conditional_probabilities = defaultdict(dict)\n",
    "\n",
    "for cls, words in word_counts.items():\n",
    "    for word in vocab:\n",
    "        count = words[word]  # This will be 0 if the word isn't in the class\n",
    "        conditional_probabilities[cls][word] = (count + 1) / (total_words_per_class[cls] + V)\n",
    "\n",
    "# 3) Define the target words and find v (count of words in to-be-found case/test case)\n",
    "x = input(\"ENTER target words:\")\n",
    "target_words = word_tokenize(x)\n",
    "# 4) Score calculation\n",
    "scores = defaultdict(float)\n",
    "\n",
    "# Calculate scores for each class\n",
    "for cls in priors.keys():\n",
    "    scores[cls] = math.log(priors[cls])  # Initialize with log prior\n",
    "\n",
    "    for word in target_words:\n",
    "        vj = word.lower()  # Convert word to lower case for comparison\n",
    "        if vj in conditional_probabilities[cls]:\n",
    "            scores[cls] += math.log(conditional_probabilities[cls][vj])\n",
    "        else:\n",
    "            # If the word is not found, assume a small probability (Laplace smoothing)\n",
    "            scores[cls] += math.log(1 / (total_words_per_class[cls] + V))\n",
    "\n",
    "# Determine the class with the highest score\n",
    "predicted_class = max(scores, key=scores.get)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Scores: {scores}\")\n",
    "print(f\"Predicted Class for target words {target_words}: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa93e9-c1dd-408e-b4ee-1676206f3cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
