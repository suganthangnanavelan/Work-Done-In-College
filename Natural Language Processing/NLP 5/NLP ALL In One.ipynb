{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b02567-52fb-42d4-8275-0f7aba99e5f9",
   "metadata": {},
   "source": [
    "# __NLP Lab__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9ecb1-e6a9-4558-adab-78b033f5a356",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f900daa-14c8-433c-8cfc-bf5b30c768ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## __Prerequisite__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0595d2-7e73-4dd3-8cef-26fdf9245ea2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Required Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518240b2-680a-41ca-afee-400b36a6ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44f95da-e3e2-415a-83c9-ca1d23807a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip -q install numpy==1.26.4 pandas==2.2.2 nltk==3.8.1 confusion_matrix==0.1 scipy==1.12.0 xlrd==2.0.1 tensorflow==2.16.1 Keras-Preprocessing==1.1.2 keras==3.6.0 matplotlib scikit-learn-1.5.2 seaborn gensim==4.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456897bb-e569-4bdb-b466-3d1b2582aa09",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddc179-44a0-44e1-a8af-1727d91df28f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### __Loading Documents From Drive__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9551fd59-fc33-4dbf-aaf0-216d862d76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"### The Impact of Social Media on Modern Communication\\n\n",
    "In the digital age, social media has revolutionized the way people communicate, offering unprecedented access to information and creating new ways to interact. Platforms like Facebook, Twitter, Instagram, and TikTok have connected individuals from across the globe, allowing for the instant exchange of ideas, images, and experiences. However, the rise of social media has also raised significant concerns about its impact on human relationships, mental health, and societal dynamics. This essay explores the positive and negative effects of social media on modern communication.\\n\n",
    "On the positive side, social media has made communication more convenient and accessible than ever before. In the past, staying in touch with friends and family required physical mail, phone calls, or face-to-face interactions. Now, platforms like Facebook and WhatsApp allow people to send messages, share updates, and make video calls at any time, from anywhere in the world. This has facilitated long-distance relationships, strengthened bonds among friends and family, and made it easier to stay connected with people who may otherwise be difficult to reach.\\n\n",
    "Moreover, social media has democratized communication, allowing individuals to express their opinions and ideas to a global audience. This has had a profound effect on activism and social movements. For example, platforms like Twitter and Instagram have played crucial roles in raising awareness about issues such as climate change, racial injustice, and political corruption. Activists can mobilize support, organize protests, and share important information in real time. The viral nature of social media also means that messages can reach millions of people in a matter of hours, making it an invaluable tool for social change.\\n\n",
    "However, social media's influence is not entirely positive. One of the primary concerns is the effect it has on face-to-face communication skills. As people spend more time interacting online, they may become less adept at having meaningful in-person conversations. Social media interactions tend to be more superficial, with users often relying on emojis, likes, or short messages rather than engaging in deep, thoughtful discussions. This can result in a decline in the quality of personal relationships, as online communication often lacks the nuances and emotional depth found in face-to-face conversations.\\n\n",
    "Another issue is the impact of social media on mental health. Studies have shown that excessive use of platforms like Instagram and Facebook can lead to feelings of isolation, anxiety, and depression. Constant comparison to others, especially when viewing curated, idealized images of other people's lives, can lead to low self-esteem and body image issues. The pressure to present a perfect life online, coupled with the fear of missing out (FOMO), can also contribute to heightened stress and dissatisfaction. Additionally, cyberbullying and online harassment have become increasingly prevalent, leading to harmful consequences for individuals, particularly teenagers.\\n\n",
    "Furthermore, social media can exacerbate the spread of misinformation. Fake news, conspiracy theories, and misleading content can spread rapidly across platforms, influencing public opinion and distorting perceptions of reality. The algorithms that govern social media platforms often prioritize content that generates engagement, meaning sensational or controversial material is more likely to be shared and seen by a wide audience. This can create echo chambers where individuals are exposed only to information that confirms their existing beliefs, reinforcing polarization and division in society.\\n\n",
    "In conclusion, social media has undeniably transformed modern communication, making it easier to connect with others and share information on a global scale. However, its impact on face-to-face interactions, mental health, and the spread of misinformation presents significant challenges. As social media continues to evolve, it is crucial that users and society as a whole strike a balance, using these platforms in ways that enhance communication while minimizing their negative effects.\"\"\"\n",
    "\n",
    "text2 = \"\"\"chair put coat, back Furniture\n",
    "chair IT department Furniture\n",
    "where here put chair Furniture\n",
    "CSE chair head Position\"\"\"\n",
    "\n",
    "with open('sample1.txt', 'w') as f1:\n",
    "    f1.write(text1)\n",
    "with open('sample2.txt', 'w') as f2:\n",
    "    f2.write(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b6f276-e07e-49cc-9a74-f9382d5f29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url3 = \"https://drive.google.com/uc?export=download&id=1_yRTLQSinbp-ePnUyLVpbRMUDBceRbMU\" #data.csv\n",
    "pd.read_csv(url3).to_csv('data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef5b55-9562-4cc2-81b7-cd2f6bd8af3a",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1703ec-42f3-4023-9c71-b3f46587ea79",
   "metadata": {},
   "source": [
    "# __2.Implement t-Test and Chi-Square test to check whether a given sequence of words is a collocation or not.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbde4a26-a923-4b2a-a20a-5010ae791ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80964614-3a9a-4ec4-8765-08c2ad506753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '#', '#', 'The', 'Impact', 'of', 'Social', 'Media', 'on', 'Modern', 'Communication', 'In', 'the', 'digital', 'age', ',', 'social', 'media', 'has', 'revolutionized', 'the', 'way', 'people', 'communicate', ',', 'offering', 'unprecedented', 'access', 'to', 'information', 'and', 'creating', 'new', 'ways', 'to', 'interact', '.', 'Platforms', 'like', 'Facebook', ',', 'Twitter', ',', 'Instagram', ',', 'and', 'TikTok', 'have', 'connected', 'individuals', 'from', 'across', 'the', 'globe', ',', 'allowing', 'for', 'the', 'instant', 'exchange', 'of', 'ideas', ',', 'images', ',', 'and', 'experiences', '.', 'However', ',', 'the', 'rise', 'of', 'social', 'media', 'has', 'also', 'raised', 'significant', 'concerns', 'about', 'its', 'impact', 'on', 'human', 'relationships', ',', 'mental', 'health', ',', 'and', 'societal', 'dynamics', '.', 'This', 'essay', 'explores', 'the', 'positive', 'and']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "f=open(\"sample1.txt\",\"r\")\n",
    "text=f.read()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "word_tokens = word_tokenize(text)\n",
    "fil_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "print(word_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba06d703-8acf-47bc-b5cf-164e1af401c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the critical value :  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9900990099009901\n",
      "['that', 'enhance', 0.9900990099009901]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def collocation(w1,w2):\n",
    "    nl=list()\n",
    "    N=len(word_tokens)\n",
    "    prob_w1=word_tokens.count(w1)/N\n",
    "    prob_w2=word_tokens.count(w2)/N\n",
    "    pop_mean=prob_w1*prob_w2\n",
    "    count_w1w2=0\n",
    "    for i in range(len(word_tokens)-1):\n",
    "        if(word_tokens[i]==w1 and word_tokens[i+1]==w2):\n",
    "            count_w1w2=count_w1w2+1\n",
    "    sam_mean=count_w1w2/N\n",
    "    t=(sam_mean-pop_mean)/(sam_mean/N)**0.5\n",
    "    cv=input(\"enter the critical value : \")\n",
    "    if(float(t) > float(cv)):\n",
    "        print(f\"hypothesis rejected thus the given words ('{w1}', '{w2}') form a collocation\")\n",
    "    print(t)\n",
    "    nl.append(w1)\n",
    "    nl.append(w2)\n",
    "    nl.append(t)\n",
    "    print(nl)\n",
    "fcol=list()\n",
    "i = random.randint(0,len(word_tokens))\n",
    "#for i in range(len(word_tokens)-1):\n",
    "w1=word_tokens[i]\n",
    "w2=word_tokens[i+1]\n",
    "fcol.append(collocation(w1,w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3fa554-68cb-445d-bd94-86c1a95b9790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the critical value :  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothesis rejected thus the given words (',', 'and') form a collocation\n",
      "14.5297284132707\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def collocation(w1,w2):\n",
    "  nl=list()\n",
    "  N=len(word_tokens)\n",
    "  count_w1=word_tokens.count(w1)\n",
    "  count_w2=word_tokens.count(w2)\n",
    "\n",
    "  Exp_w1w2= ((count_w1*count_w2)/N)  \n",
    "  Exp_w1nw2= ((count_w1*(N-count_w2))/N)\n",
    "  Exp_nw1w2= (((N-count_w1)*count_w2)/N)\n",
    "  Exp_nw1nw2= (((N-count_w1)*(N-count_w2)/N))\n",
    "\n",
    "  j=0\n",
    "  count_w1w2=0\n",
    "  for i in range(len(word_tokens)):\n",
    "    if(word_tokens[i]==w1 and word_tokens[i+1]==w2):\n",
    "      count_w1w2=count_w1w2+1\n",
    "  count_w1w2=j\n",
    "  \n",
    "  Obs_w1w2=count_w1w2\n",
    "  Obs_w1nw2=count_w1-count_w1w2\n",
    "  Obs_nw1w2=count_w2-count_w1w2\n",
    "  Obs_nw1nw2=N-count_w1w2\n",
    "\n",
    "  X= (((Obs_w1w2-Exp_w1w2)**2)/Exp_w1w2) + (((Obs_w1nw2-Exp_w1nw2)**2)/Exp_w1nw2) + (((Obs_nw1w2-Exp_nw1w2)**2)/Exp_nw1w2) + (((Obs_nw1nw2-Exp_nw1nw2)**2)/Exp_nw1nw2)\n",
    "  cv=int(input(\"enter the critical value : \"))\n",
    "  if(float(X) > float(cv)):\n",
    "    print(f\"hypothesis rejected thus the given words ('{w1}', '{w2}') form a collocation\")\n",
    "    print(X)\n",
    "    nl.append(w1)\n",
    "    nl.append(w2)\n",
    "    nl.append(X)\n",
    "  else:print(f\"hypothesis accepted thus the given words ('{w1}', '{w2}') does not form a collocation\")\n",
    "  return nl\n",
    "    \n",
    "fcol=list()\n",
    "i = random.randint(0,len(word_tokens))\n",
    "#for i in range(len(word_tokens)-1):\n",
    "w1=word_tokens[i]\n",
    "w2=word_tokens[i+1]\n",
    "fcol.append(collocation(w1,w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df14fc7-f10c-49eb-baa5-6011c270b0b5",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d5490-3d7e-4476-9d3a-5055dd0127b3",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b961cee-e99f-4b97-a287-a92938500df5",
   "metadata": {},
   "source": [
    "# __3.WSD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "378d2a7f-b3ec-4228-b436-58e4c551adf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5418f7e-eb6c-4663-af6e-1f05e5e3ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b902b1ab-918a-47d9-b57c-3ab140756a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"sample2.txt\")\n",
    "text=f.read()\n",
    "data=text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7636c47c-2f57-49ff-ab65-f326ed209def",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=list()\n",
    "for i in data:\n",
    "  tokens=word_tokenize(i)\n",
    "  ds.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ed7e570-1bb2-447b-9762-4db619615a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chair', 'put', 'coat', ',', 'back', 'Furniture']\n",
      "['chair', 'IT', 'department', 'Furniture']\n",
      "['where', 'here', 'put', 'chair', 'Furniture']\n",
      "['CSE', 'chair', 'head', 'Position']\n"
     ]
    }
   ],
   "source": [
    "for i in ds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b5525b9-9c64-4604-ad57-37d6674bfee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter sentence: coat back chair\n",
      "enter word to find sense: chair\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0 -3.0\n"
     ]
    }
   ],
   "source": [
    "test_sen = input(\"enter sentence:\")\n",
    "test_sen = test_sen.split(\" \")\n",
    "sense_word = input(\"enter word to find sense:\")\n",
    "\n",
    " #let us assume that there are two senses(furniture,position)\n",
    "cf = float(text.count(\"Furniture\"))\n",
    "cp = float(text.count(\"Position\"))\n",
    "#12 unique vocabularies\n",
    "scoref = math.log2((cf+1)/(cf+cp+12))\n",
    "scorep = math.log2((cp+1)/(cf+cp+12))\n",
    "print(scoref,scorep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11de99e1-3654-4c51-acf3-fd1f9daeaa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fur = list()\n",
    "pos = list()\n",
    "for i in ds:\n",
    "  if(\"Furniture\" in i):\n",
    "    fur.append(i)\n",
    "  else:\n",
    "    pos.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69385d62-91b3-4df5-a11d-7a60c73401e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:  1\n",
      "pf:  0.13333333333333333\n",
      "p:  0\n",
      "pp:  0.07692307692307693\n",
      "final scoref -4.906890595608519\n",
      "final scorep -6.700439718141093\n",
      "f:  1\n",
      "pf:  0.13333333333333333\n",
      "p:  0\n",
      "pp:  0.07692307692307693\n",
      "final scoref -4.906890595608519\n",
      "final scorep -6.700439718141093\n",
      "f:  3\n",
      "pf:  0.26666666666666666\n",
      "p:  1\n",
      "pp:  0.15384615384615385\n",
      "final scoref -3.9068905956085187\n",
      "final scorep -5.700439718141093\n"
     ]
    }
   ],
   "source": [
    "for word in test_sen:\n",
    "  p=0\n",
    "  f=0\n",
    "  for i in fur:\n",
    "    if(word in i):\n",
    "      f=f+1\n",
    "  for i in pos:\n",
    "    if(word in i):\n",
    "      p=p+1\n",
    "  print(\"f: \",f)\n",
    "  print(\"pf: \",(f+1)/(cf+12))\n",
    "  print(\"p: \",p)\n",
    "  print(\"pp: \",(p+1)/(cp+12))\n",
    "  final_scoref= scoref + math.log2(((f+1)/(cf+12)))\n",
    "  print(\"final scoref\",final_scoref)\n",
    "  final_scorep= scorep + math.log2(((p+1)/(cp+12)))\n",
    "  print(\"final scorep\",final_scorep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49bba1b5-0bfb-4dc8-92a3-0560ae2f01e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the given  chair is of sense Furniture in the given sentence\n"
     ]
    }
   ],
   "source": [
    "if(final_scorep > final_scoref):\n",
    "  print(\"the given \",sense_word,\"is of sense Position in the given sentence\")\n",
    "else:\n",
    "  print(\"the given \",sense_word,\"is of sense Furniture in the given sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1beacf-0473-4bfa-9669-1be8bf496013",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a83d22-f60c-481c-b992-1aff240cb7dc",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb94a4-daca-45d7-89fc-022550c54cb0",
   "metadata": {},
   "source": [
    "# __4.Hindle & Rooth__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb6ea3e5-356f-49e6-add7-9ea4730f9f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the preposition :  with\n",
      "Enter the noun :  phone\n",
      "Enter the Verb :  wait\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('saw', 'phone')]\n",
      "[('went', 'meeting'), ('meeting', 'yesterday')]\n",
      "[('told', 'man'), ('man', 'wait')]\n",
      "[('gave', 'book')]\n",
      "[('saw', 'cat')]\n",
      "{'saw': 2, 'phone': 1, 'went': 1, 'meeting': 1, 'yesterday': 1, 'told': 1, 'man': 1, 'wait': 1, 'gave': 1, 'book': 1, 'cat': 1}\n",
      "{'phone': 1, 'yesterday': 2, 'wait': 2, 'book': 1, 'cat': 1}\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "no attachment\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "\n",
    "prep = input(\"Enter the preposition : \")\n",
    "noun = input(\"Enter the noun : \")\n",
    "verb = input(\"Enter the Verb : \")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "sentences = [\n",
    "    \"Saw the phone with me.\",\n",
    "    \"Went to the meeting yesterday.\",\n",
    "    \"Told the man to wait.\",\n",
    "    \"Gave the book to her.\",\n",
    "    \"Saw the cat with her.\"\n",
    "]\n",
    "\n",
    "unigram = {}\n",
    "bigram = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token.lower() for token in tokens if token not in string.punctuation and token not in stop_words]\n",
    "    bigr = list(bigrams(tokens))\n",
    "    print(bigr)\n",
    "    for word in tokens:\n",
    "        if word in unigram:\n",
    "            unigram[word]+=1\n",
    "        else:\n",
    "            unigram[word] = 1\n",
    "    for bi in bigr:\n",
    "        if word in bigram:\n",
    "            bigram[word]+=1\n",
    "        else:\n",
    "            bigram[word] = 1\n",
    "\n",
    "print(unigram)\n",
    "print(bigram)\n",
    "\n",
    "bigram.setdefault((noun.lower(),prep.lower()),0)\n",
    "bigram.setdefault((verb.lower(),prep.lower()),0)\n",
    "unigram.setdefault(noun.lower(),0)\n",
    "unigram.setdefault(verb.lower(),0)\n",
    "unigram.setdefault(prep.lower(),0)\n",
    "\n",
    "print(bigram[(noun.lower(),prep.lower())])\n",
    "print(unigram[noun.lower()])\n",
    "print(bigram[(verb.lower(),prep.lower())])\n",
    "print(unigram[verb.lower()])\n",
    "\n",
    "pn = bigram[(noun.lower(),prep.lower())]/unigram[noun.lower()]\n",
    "pv = bigram[(verb.lower(),prep.lower())]/unigram[verb.lower()]\n",
    "\n",
    "try: \n",
    "    lam = math.log2((pv*(1-pn))/pn)\n",
    "    if lam < 1:\n",
    "        print(\"attached with noun\")\n",
    "    else:\n",
    "        print(\"attached with verb\")\n",
    "except ZeroDivisionError as e:\n",
    "    print(\"no attachment\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09381c48-2e0b-4324-9535-d32bc12de7e1",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466a4e0-258b-4f15-9313-0a66120c788f",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a72f92-69f9-4b2c-82b8-6ac18f1e7268",
   "metadata": {},
   "source": [
    "# __5.Forward Backward Probability of Observed Sequence__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0c4e603-f344-4cc6-bed7-870a28b98725",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['cp', 'ip']\n",
    "ip = [1.0, 0.0]\n",
    "stp = [[0.7, 0.3], [0.5, 0.5]]\n",
    "op = ['lem', 'ice', 'cola']\n",
    "opp = [[0.3, 0.1, 0.6], [0.2, 0.7, 0.1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "674dac88-a26e-4af4-93cf-d8467f98a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_alg():\n",
    "    alpha = []\n",
    "    for _ in range(len(op)+1):\n",
    "        alpha.append([0,0])\n",
    "\n",
    "    '''alpha[0][0] = 1.0\n",
    "    alpha[0][1] = 0.0\n",
    "    #print(alpha)'''\n",
    "    \n",
    "    for i in range(len(alpha)):\n",
    "        if i == 0:\n",
    "            alpha[i][0] = ip[0]\n",
    "            alpha[i][1] = ip[1]\n",
    "            continue\n",
    "        \n",
    "        alpha[i][0] = float(float(stp[0][0]*opp[0][i-1]*alpha[i-1][0]) + float(stp[1][0]*opp[1][i-1]*alpha[i-1][1]))\n",
    "\n",
    "        alpha[i][1] = float(float(stp[1][1]*opp[1][i-1]*alpha[i-1][1]) + float(stp[0][1]*opp[0][i-1]*alpha[i-1][0]))\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4ffe33-f516-40ae-a721-0afee9db9f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0]\n",
      "[0.21, 0.09]\n",
      "[0.0462, 0.0378]\n",
      "[0.021293999999999997, 0.010206]\n",
      "\n",
      "Probability of whole sequence :  0.0315\n"
     ]
    }
   ],
   "source": [
    "alpha = forward_alg()\n",
    "pos_alpha = 0.0\n",
    "for i in alpha:\n",
    "    pos_alpha = sum(i)\n",
    "    print(i)\n",
    "    \n",
    "print(\"\\nProbability of whole sequence : \",pos_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81593ceb-34b6-4dff-82f3-d04dd379d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_alg():\n",
    "    beta = []\n",
    "    for _ in range(len(op)+1):\n",
    "        beta.append([0.0,0.0])\n",
    "\n",
    "    beta[len(op)][0] = 1.0\n",
    "    beta[len(op)][1] = 1.0\n",
    "    #print(beta)\n",
    "\n",
    "    for i in range(len(op)-1,-1,-1):\n",
    "        \n",
    "        beta[i][0] = float(float(stp[0][0]*opp[0][i]*beta[i+1][0]) + float(stp[0][1]*opp[0][i]*beta[i+1][1]))\n",
    "\n",
    "        beta[i][1] = float(float(stp[1][1]*opp[1][i]*beta[i+1][1]) + float(stp[1][0]*opp[1][i]*beta[i+1][0]))\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d982baed-e8e7-4abf-935c-f2f50ac3e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0315, 0.029]\n",
      "[0.045, 0.245]\n",
      "[0.6, 0.1]\n",
      "[1.0, 1.0]\n",
      "\n",
      "Probability of whole sequence :  0.0315\n"
     ]
    }
   ],
   "source": [
    "beta = backward_alg()\n",
    "for i in beta:\n",
    "    print(i)\n",
    "pos_beta = beta[0][0]*ip[0] + beta[0][1]*ip[1]\n",
    "print(\"\\nProbability of whole sequence : \",pos_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cae044-9658-43a8-b973-1e64e83c6390",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041ec2e-f279-4b0c-950e-038143a802db",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeabfe2-283f-47eb-98b1-250eedb203d6",
   "metadata": {},
   "source": [
    "# __6.Viterbi Algorithm (Best State Sequence)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "054ed639-31cd-489c-87d7-6f910bdff677",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['cp', 'ip']\n",
    "ip = [1.0, 0.0]\n",
    "stp = [[0.7, 0.3], [0.5, 0.5]]\n",
    "op = ['lem', 'ice', 'cola']\n",
    "opp = [[0.3, 0.1, 0.6], [0.2, 0.7, 0.1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11e5e731-7595-450d-866e-cdd0f488d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algorithm(op):\n",
    "    delta = []\n",
    "    psi = []\n",
    "\n",
    "    # Initialize the delta and psi matrices\n",
    "    for _ in range(len(op)):\n",
    "        delta.append([0.0, 0.0])\n",
    "        psi.append([0, 0])\n",
    "\n",
    "    # Initialization step\n",
    "    for i in range(len(states)):\n",
    "        delta[0][i] = ip[i] * opp[i][0]\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, len(op)):\n",
    "        for j in range(len(states)):\n",
    "            max_prob = 0.0\n",
    "            max_state = 0\n",
    "\n",
    "            for i in range(len(states)):\n",
    "                prob = delta[t - 1][i] * stp[i][j] * opp[j][t]\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_state = i\n",
    "\n",
    "            delta[t][j] = max_prob\n",
    "            psi[t][j] = max_state\n",
    "\n",
    "    # Termination step\n",
    "    best_path_prob = max(delta[-1])\n",
    "    best_last_state = delta[-1].index(best_path_prob)\n",
    "\n",
    "    # Backtrack to find the best tag sequence\n",
    "    best_path = [best_last_state]\n",
    "    for t in range(len(op) - 1, 0, -1):\n",
    "        best_last_state = psi[t][best_last_state]\n",
    "        best_path.insert(0, best_last_state)\n",
    "\n",
    "    return best_path, best_path_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a2c4fc4-ba8e-4d4f-9f58-8f225919508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tag Sequence: ['cp', 'ip', 'cp']\n",
      "Probability of the Best Tag Sequence: 0.0189\n"
     ]
    }
   ],
   "source": [
    "# Call the Viterbi algorithm\n",
    "best_tag_sequence, probability = viterbi_algorithm(op)\n",
    "\n",
    "# Print the best tag sequence and its probability\n",
    "print(\"Best Tag Sequence:\", [states[i] for i in best_tag_sequence])\n",
    "print(\"Probability of the Best Tag Sequence:\", probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31e1a8-6821-498d-9895-557a43205f9d",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666040aa-05c5-4815-adf5-c64ffab0e94c",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77249cef-b6b6-46e8-afb2-1a83730855a0",
   "metadata": {},
   "source": [
    "# __7.PCFG Using CYK__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcbaa054-0d06-4ac6-86f6-817121de6640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside probability of S: 21.400000000000002\n",
      "Inside probability of NP: 18.479999999999997\n",
      "Inside probability of VP: 17.120000000000005\n",
      "Inside probability of Det: 0.0\n",
      "Inside probability of N: 0.0\n",
      "Inside probability of V: 0.0\n",
      "Total inside probability: 21.400000000000002\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "def cyk_algorithm(words, pcfg_rules):\n",
    "    n = len(words)\n",
    "    table = [[defaultdict(float) for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    # Initialization\n",
    "    for i, word in enumerate(words):\n",
    "        for nt, (prob, terminals) in pcfg_rules.items():\n",
    "            if word in terminals:\n",
    "                table[i][i][nt] = prob\n",
    "\n",
    "    # CYK Algorithm\n",
    "    for length in range(2, n + 1):\n",
    "        for i in range(n - length + 1):\n",
    "            j = i + length - 1\n",
    "            for k in range(i, j):\n",
    "                for A, (prob_A, _) in pcfg_rules.items():\n",
    "                    for B, (prob_B, _) in pcfg_rules.items():\n",
    "                        for C in table[i][k]:\n",
    "                            for D in table[k + 1][j]:\n",
    "                                prob = prob_A * prob_B * pcfg_rules[A][1].count(C) * pcfg_rules[B][1].count(D)\n",
    "                                table[i][j][A] += prob\n",
    "\n",
    "    return table\n",
    "\n",
    "# Example PCFG rules (non-terminal -> (probability, [productions]))\n",
    "pcfg_rules = {\n",
    "    'S': (1.0, ['NP', 'VP']),\n",
    "    'NP': (0.7, ['Det', 'N']),\n",
    "    'VP': (0.8, ['V', 'NP']),\n",
    "    'Det': (1.0, ['the']),\n",
    "    'N': (0.6, ['cat', 'dog']),\n",
    "    'V': (0.9, ['chased'])\n",
    "}\n",
    "\n",
    "# Example input sentence\n",
    "words = ['the', 'cat', 'chased', 'the', 'dog']\n",
    "\n",
    "# Call CYK algorithm to get inside probabilities\n",
    "table = cyk_algorithm(words, pcfg_rules)\n",
    "\n",
    "# Inside probabilities for non-terminals in the top cell of the table\n",
    "inside_probabilities = table[0][-1]\n",
    "\n",
    "# Print inside probabilities\n",
    "for nt, prob in inside_probabilities.items():\n",
    "    print(f'Inside probability of {nt}: {prob}')\n",
    "\n",
    "# Total inside probability (probability of the whole sentence)\n",
    "total_probability = inside_probabilities['S']\n",
    "print(f'Total inside probability: {total_probability}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba442a3e-6e62-44d4-8442-d6b56eda9218",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63b728",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b467efb2",
   "metadata": {},
   "source": [
    "# __7.PCFG Parse Tree__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87e154",
   "metadata": {},
   "source": [
    "LHS `->` RHS  \n",
    "Element1 `|` Element2 `|` Element3  \n",
    "`NP PP [0.4]` -- non-terminal symbol   \n",
    "`'he' [0.6]` -- terminal symbol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fed8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammarstring = \"\"\"\n",
    "S -> NP VP [1.0]\n",
    "NP -> NP PP [0.4] | 'he' [0.1] | 'dessert' [0.3] | 'lunch' [0.1] | 'saw' [0.1]\n",
    "PP -> Pre NP [1.0]\n",
    "VP -> Verb NP [0.3] | VP PP [0.7]\n",
    "Pre -> 'with' [0.6] | 'in' [0.4]\n",
    "Verb -> 'ate' [0.7] | 'saw' [0.3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1623f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PCFG, InsideChartParser \n",
    "# Remember InsideChartParser\n",
    "\n",
    "grammar = PCFG.fromstring(grammarstring)\n",
    "\n",
    "parser = InsideChartParser(grammar=grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc73b676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'saw', 'lunch', 'with', 'dessert']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"he saw lunch with dessert\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ad4c283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               S                       \n",
      "  _____________|____                    \n",
      " |                  VP                 \n",
      " |         _________|________           \n",
      " |        VP                 PP        \n",
      " |    ____|____          ____|_____     \n",
      " NP Verb       NP      Pre         NP  \n",
      " |   |         |        |          |    \n",
      " he saw      lunch     with     dessert\n",
      "\n",
      "Prob:  0.00011339999999999999\n",
      "           S                       \n",
      "  _________|____                    \n",
      " |              VP                 \n",
      " |    __________|___                \n",
      " |   |              NP             \n",
      " |   |      ________|____           \n",
      " |   |     |             PP        \n",
      " |   |     |         ____|_____     \n",
      " NP Verb   NP      Pre         NP  \n",
      " |   |     |        |          |    \n",
      " he saw  lunch     with     dessert\n",
      "\n",
      "Prob:  6.480000000000002e-05\n"
     ]
    }
   ],
   "source": [
    "trees = parser.parse(tokens)\n",
    "\n",
    "for tree in trees:\n",
    "    tree.pretty_print() # Remember this\n",
    "    print(\"Prob: \", tree.prob())# .prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dafed0",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792f73a-c8d0-41b6-9e24-0d82440167cb",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd44fd-19a5-4c72-93d8-bd000f92fce9",
   "metadata": {},
   "source": [
    "# __8.Test Classification using BOW & TF-IDF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e288984d-5ed7-4349-bb04-c69da5dba922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0da6600d-5299-4999-ac05-0e272aa41813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Bag of Words (BoW)...\n",
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.3333 - loss: 0.7565\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3333 - loss: 0.7452\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3333 - loss: 0.7609\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3333 - loss: 0.7832\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1667 - loss: 0.8181     \n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1667 - loss: 0.8078.886\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3333 - loss: 0.7512\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3333 - loss: 0.7415\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3333 - loss: 0.7320\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3333 - loss: 0.6980\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - accuracy: 0.0000e+00 - loss: 0.8013\n",
      "BoW Model Accuracy: 0.00\n",
      "Training with TF-IDF...\n",
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.3333 - loss: 0.7081\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1667 - loss: 0.7425     \n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3333 - loss: 0.7165\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3333 - loss: 0.6989\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1667 - loss: 0.7169.718\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1667 - loss: 0.7218    \n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1667 - loss: 0.7183.72\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.3333 - loss: 0.6927\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1667 - loss: 0.7046      \n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1667 - loss: 0.7117718\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - accuracy: 0.0000e+00 - loss: 0.8679\n",
      "TF-IDF Model Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "data = {'text': ['I love programming', 'Python is great', 'I enjoy machine learning',\n",
    "                 'TensorFlow is a powerful tool', 'AI is the future'],\n",
    "        'label': ['positive', 'positive', 'positive', 'positive', 'neutral']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Option 1: Bag of Words (BoW)\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(X_train).toarray()\n",
    "X_test_bow = vectorizer_bow.transform(X_test).toarray()\n",
    "\n",
    "# Option 2: TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test).toarray()\n",
    "\n",
    "# Build a simple neural network with TensorFlow\n",
    "def build_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification (positive or neutral)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Training with Bag of Words (BoW)...\")\n",
    "model_bow = build_model(X_train_bow.shape[1])\n",
    "model_bow.fit(X_train_bow, y_train, epochs=10, batch_size=2, verbose=1)\n",
    "\n",
    "loss, accuracy = model_bow.evaluate(X_test_bow, y_test)\n",
    "print(f'BoW Model Accuracy: {accuracy:.2f}')\n",
    "\n",
    "print(\"Training with TF-IDF...\")\n",
    "model_tfidf = build_model(X_train_tfidf.shape[1])\n",
    "model_tfidf.fit(X_train_tfidf, y_train, epochs=10, batch_size=2, verbose=1)\n",
    "\n",
    "\n",
    "loss, accuracy = model_tfidf.evaluate(X_test_tfidf, y_test)\n",
    "print(f'TF-IDF Model Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e1891-59fb-426a-9fcd-afa0e2e28898",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f82a3-a9da-4139-be43-ebc5b071e342",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f3bb1-38a2-40bf-bea9-08452f33f615",
   "metadata": {},
   "source": [
    "# __9.Word2Vec Semantic Similarity Between Words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2e8ca01-88e6-4574-ac39-df95af94d592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'programming':\n",
      "the: 0.1783\n",
      "i: 0.1607\n",
      "a: 0.1056\n",
      "great: 0.0922\n",
      "python: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sugan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Implement word2vec model to explore the semantic similarity between the words.\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentences = ['I love programming', 'Python is great', 'I enjoy machine learning',\n",
    "             'TensorFlow is a powerful tool', 'AI is the future']\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "\n",
    "model_w2v = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "#  word vectors (semantic similarity)\n",
    "word = 'programming'\n",
    "similar_words = model_w2v.wv.most_similar(word, topn=5)\n",
    "print(f\"Words most similar to '{word}':\")\n",
    "for sim_word, sim_score in similar_words:\n",
    "    print(f\"{sim_word}: {sim_score:.4f}\")\n",
    "model_w2v.save(\"word2vec_model.bin\")\n",
    "#loaded_model = Word2Vec.load(\"word2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb5d06-1c65-47a3-9e07-d63c718c8e58",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc390b-f621-49c0-b6ee-b78153b1edfe",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae676c70-b4c6-4b2d-8fa0-c2e34a93130a",
   "metadata": {},
   "source": [
    "# __10.Text Generation using LSTM__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe84ea98-5c65-41ca-bcd0-3c24ce1ccdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4f5743d-c32d-45d7-967f-374ad4c99a09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 3.0909\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.0617 \n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.0476 \n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.0184\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.9871 \n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.9835 \n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.9310 \n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8329 \n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.9373 \n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8274 \n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.9145 \n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8772 \n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.8658\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8369 \n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.9302\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.8489\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.7994\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8294 \n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.7825\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.7956\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8213 \n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.8103\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7470 \n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8715 \n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7876 \n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7771 \n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8300 \n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8605 \n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8213 \n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.8108 \n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.8240\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.7978\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7238 \n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.8862\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.7856\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8229 \n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7645 \n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7493 \n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7475 \n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.7241\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7854 \n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7840 \n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7436 \n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7419 \n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7192 \n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7753 \n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6806 \n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7320 \n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6792 \n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6408 \n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7425 \n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.6685 \n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7425 \n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6131 \n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.5712\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5494 \n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4884 \n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5221 \n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5235 \n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5362 \n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.6451 \n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5194 \n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.4620 \n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5619 \n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5013 \n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4275 \n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5200 \n",
      "Epoch 68/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3822 \n",
      "Epoch 69/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3593 \n",
      "Epoch 70/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3916 \n",
      "Epoch 71/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3572 \n",
      "Epoch 72/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3626 \n",
      "Epoch 73/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.2668 \n",
      "Epoch 74/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2636 \n",
      "Epoch 75/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3261 \n",
      "Epoch 76/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.3806\n",
      "Epoch 77/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.2650\n",
      "Epoch 78/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2802 \n",
      "Epoch 79/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.1351 \n",
      "Epoch 80/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.1741\n",
      "Epoch 81/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2270 \n",
      "Epoch 82/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2480 \n",
      "Epoch 83/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.3334 \n",
      "Epoch 84/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.2408 \n",
      "Epoch 85/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.1549 \n",
      "Epoch 86/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.0900 \n",
      "Epoch 87/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.1039 \n",
      "Epoch 88/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.9848 \n",
      "Epoch 89/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.1165 \n",
      "Epoch 90/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.9919 \n",
      "Epoch 91/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.9953 \n",
      "Epoch 92/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8658 \n",
      "Epoch 93/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.9966\n",
      "Epoch 94/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.9468 \n",
      "Epoch 95/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8637 \n",
      "Epoch 96/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.9122 \n",
      "Epoch 97/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.9066 \n",
      "Epoch 98/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8174 \n",
      "Epoch 99/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8496 \n",
      "Epoch 100/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.8804 \n",
      "This is a etmpllo  nTMamppl oo  tttt  oeeTampl oo  etampllo \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Define your text data\n",
    "text = \"This is a sample text for text generation using LSTM.\"\n",
    "\n",
    "# Preprocess the text data\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {char: i for i, char in enumerate(chars)}\n",
    "int_to_char = {i: char for i, char in enumerate(chars)}\n",
    "num_chars = len(chars)\n",
    "seq_length = 10\n",
    "\n",
    "data_X = []\n",
    "data_y = []\n",
    "for i in range(0, len(text) - seq_length):\n",
    "    seq_in = text[i:i+seq_length]\n",
    "    seq_out = text[i+seq_length]\n",
    "    data_X.append([char_to_int[char] for char in seq_in])\n",
    "    data_y.append(char_to_int[seq_out])\n",
    "\n",
    "X = np.reshape(data_X, (len(data_X), seq_length, 1))\n",
    "X = X / float(num_chars)\n",
    "y = np.eye(num_chars)[data_y]\n",
    "\n",
    "# Define and train the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=16)\n",
    "\n",
    "# Generate text using the trained model\n",
    "start_seq = \"This is a \"\n",
    "generated_text = start_seq\n",
    "\n",
    "for _ in range(50):\n",
    "    x = np.reshape([char_to_int[char] for char in start_seq], (1, len(start_seq), 1))\n",
    "    x = x / float(num_chars)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    generated_text += result\n",
    "    start_seq = start_seq[1:] + result\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2b34d-753c-46fe-af3b-0c30fea31eb9",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bbbbcf",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a157b",
   "metadata": {},
   "source": [
    "# __11.Machine Translation System__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f975075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1109aec3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "193e3bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Run!</td>\n",
       "      <td>¡Corre!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Corred.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who?</td>\n",
       "      <td>¿Quién?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>¡Fuego!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>¡Incendio!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english     spanish\n",
       "0     Go.         Ve.\n",
       "1     Go.       Vete.\n",
       "2     Go.       Vaya.\n",
       "3     Go.     Váyase.\n",
       "4     Hi.       Hola.\n",
       "5    Run!     ¡Corre!\n",
       "6    Run.     Corred.\n",
       "7    Who?     ¿Quién?\n",
       "8   Fire!     ¡Fuego!\n",
       "9   Fire!  ¡Incendio!"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21e22ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['english', 'spanish'], dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45254233",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = set(' '.join(df['english'][25:125]))\n",
    "target_vocab = set(' '.join(df['spanish'][25:125]))\n",
    "source_vocab_size = len(source_vocab)\n",
    "target_vocab_size = len(target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1d93d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r', 'C', '1', 'g', 'c', 'D', 'W', 'p', \"'\", 'R', 'y', '!', 'o', 'n', 'H', 'u', 'O', 'I', 'e', '.', 'm', 'B', 't', 'N', 'i', 'L', 'A', 's', '9', 'T', 'a', 'k', 'w', 'S', 'l', 'd', 'x', 'G', 'f', 'b', '?', 'h', 'q', ' '}\n",
      "{'r', 'C', '¡', ',', 'g', 'c', 'D', 'Y', 'p', '¿', 'R', '!', 'y', 'o', 'n', 'H', 'í', 'u', 'é', 'O', 'É', 'E', 'I', 'á', 'e', '.', 'm', 'B', 't', 'N', 'i', 'Á', 'L', 'ú', 'A', 's', 'V', 'T', 'a', 'M', 'j', 'P', 'S', 'd', 'l', 'z', 'ó', 'G', 'Ó', 'f', 'b', '?', 'h', 'v', 'q', 'U', ' '}\n"
     ]
    }
   ],
   "source": [
    "print(source_vocab)\n",
    "print(target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "658cc282",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_char_to_int = {char: idx for idx, char in enumerate(source_vocab)}\n",
    "target_char_to_int = {char: idx for idx, char in enumerate(target_vocab)}\n",
    "source_int_to_char = {idx: char for char, idx in source_char_to_int.items()}\n",
    "target_int_to_char = {idx: char for char, idx in target_char_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdee83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text sequences to integer sequences\n",
    "source_sequences = [[source_char_to_int[char] for char in text] for text in df['english'][25:125]]\n",
    "target_sequences = [[target_char_to_int[char] for char in text] for text in df['spanish'][25:125]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ad12206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "max_sequence_length = max(len(seq) for seq in source_sequences)\n",
    "source_sequences = tf.keras.preprocessing.sequence.pad_sequences(source_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bb2d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "input_shape = (max_sequence_length, source_vocab_size)\n",
    "output_shape = (max_sequence_length, target_vocab_size)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Embedding layer with a larger dimension for richer word representations\n",
    "    tf.keras.layers.Embedding(source_vocab_size, 512, input_length=max_sequence_length),\n",
    "\n",
    "    # First Bidirectional RNN layer with dropout and L2 regularization\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(512, return_sequences=True, \n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(0.001))),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "    # Second RNN layer with higher units and dropout\n",
    "    tf.keras.layers.SimpleRNN(512, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "    # Third RNN layer for more complex patterns\n",
    "    tf.keras.layers.SimpleRNN(256, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    # Dense layer for more feature learning\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "\n",
    "    # Output layer to map to the target vocabulary\n",
    "    tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1973254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the target sequences\n",
    "target_sequences_one_hot = np.array([tf.keras.utils.to_categorical(seq, num_classes=target_vocab_size) for seq in target_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cba9a632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 98ms/step - accuracy: 0.0332 - loss: 5.0551\n",
      "Epoch 2/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.1902 - loss: 4.3524\n",
      "Epoch 3/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.1941 - loss: 4.0500\n",
      "Epoch 4/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.2376 - loss: 3.7719\n",
      "Epoch 5/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.3000 - loss: 3.5977\n",
      "Epoch 6/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2775 - loss: 3.4530\n",
      "Epoch 7/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.3259 - loss: 3.2802\n",
      "Epoch 8/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.3594 - loss: 3.1655\n",
      "Epoch 9/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.3916 - loss: 2.9952\n",
      "Epoch 10/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4072 - loss: 2.9167\n",
      "Epoch 11/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.4588 - loss: 2.7537\n",
      "Epoch 12/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.4680 - loss: 2.6349\n",
      "Epoch 13/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5005 - loss: 2.5371\n",
      "Epoch 14/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5058 - loss: 2.4396\n",
      "Epoch 15/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5176 - loss: 2.3706\n",
      "Epoch 16/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5162 - loss: 2.3273\n",
      "Epoch 17/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5503 - loss: 2.1850\n",
      "Epoch 18/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5601 - loss: 2.1380\n",
      "Epoch 19/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5955 - loss: 2.0653\n",
      "Epoch 20/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5852 - loss: 2.0299\n",
      "Epoch 21/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5741 - loss: 1.9942\n",
      "Epoch 22/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6189 - loss: 1.8572\n",
      "Epoch 23/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6310 - loss: 1.8258\n",
      "Epoch 24/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6203 - loss: 1.7997\n",
      "Epoch 25/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.6476 - loss: 1.7557\n",
      "Epoch 26/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6358 - loss: 1.6978\n",
      "Epoch 27/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6422 - loss: 1.6589\n",
      "Epoch 28/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.6605 - loss: 1.6016\n",
      "Epoch 29/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6668 - loss: 1.5838\n",
      "Epoch 30/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6751 - loss: 1.5420\n",
      "Epoch 31/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6892 - loss: 1.4734\n",
      "Epoch 32/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6651 - loss: 1.5056\n",
      "Epoch 33/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6720 - loss: 1.5164\n",
      "Epoch 34/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.6524 - loss: 1.5097\n",
      "Epoch 35/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7001 - loss: 1.4014\n",
      "Epoch 36/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6817 - loss: 1.4253\n",
      "Epoch 37/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6706 - loss: 1.3971\n",
      "Epoch 38/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6737 - loss: 1.3812\n",
      "Epoch 39/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.6513 - loss: 1.4100\n",
      "Epoch 40/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7072 - loss: 1.3206\n",
      "Epoch 41/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7014 - loss: 1.2956\n",
      "Epoch 42/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.6846 - loss: 1.2950\n",
      "Epoch 43/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.6892 - loss: 1.2917\n",
      "Epoch 44/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.6980 - loss: 1.2683\n",
      "Epoch 45/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.6925 - loss: 1.2881\n",
      "Epoch 46/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7074 - loss: 1.2025\n",
      "Epoch 47/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7072 - loss: 1.2442\n",
      "Epoch 48/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.6908 - loss: 1.2082\n",
      "Epoch 49/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.6817 - loss: 1.2191\n",
      "Epoch 50/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7042 - loss: 1.1976\n",
      "Epoch 51/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6920 - loss: 1.1916\n",
      "Epoch 52/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.7014 - loss: 1.1434\n",
      "Epoch 53/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.6987 - loss: 1.1585\n",
      "Epoch 54/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7030 - loss: 1.1344\n",
      "Epoch 55/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6830 - loss: 1.1507\n",
      "Epoch 56/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6954 - loss: 1.1113\n",
      "Epoch 57/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7101 - loss: 1.0874\n",
      "Epoch 58/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7162 - loss: 1.0849\n",
      "Epoch 59/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7182 - loss: 1.0639\n",
      "Epoch 60/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.6918 - loss: 1.0981\n",
      "Epoch 61/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7063 - loss: 1.0996\n",
      "Epoch 62/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.7042 - loss: 1.1236\n",
      "Epoch 63/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7015 - loss: 1.0795\n",
      "Epoch 64/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7023 - loss: 1.0363\n",
      "Epoch 65/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6922 - loss: 1.0601\n",
      "Epoch 66/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.6931 - loss: 1.0601\n",
      "Epoch 67/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6897 - loss: 1.0596\n",
      "Epoch 68/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6944 - loss: 1.0643\n",
      "Epoch 69/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6896 - loss: 1.0459\n",
      "Epoch 70/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6990 - loss: 1.0237\n",
      "Epoch 71/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7130 - loss: 0.9944\n",
      "Epoch 72/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7300 - loss: 0.9652\n",
      "Epoch 73/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.6916 - loss: 1.0241\n",
      "Epoch 74/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7275 - loss: 0.9508\n",
      "Epoch 75/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6975 - loss: 1.0039\n",
      "Epoch 76/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7142 - loss: 0.9610\n",
      "Epoch 77/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7171 - loss: 0.9440\n",
      "Epoch 78/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7109 - loss: 0.9677\n",
      "Epoch 79/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6848 - loss: 0.9847\n",
      "Epoch 80/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6812 - loss: 0.9893\n",
      "Epoch 81/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7080 - loss: 0.9469\n",
      "Epoch 82/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.7026 - loss: 0.9758\n",
      "Epoch 83/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7215 - loss: 0.9237\n",
      "Epoch 84/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7120 - loss: 0.9096\n",
      "Epoch 85/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7119 - loss: 0.9188\n",
      "Epoch 86/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7285 - loss: 0.8869\n",
      "Epoch 87/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.7050 - loss: 0.9303\n",
      "Epoch 88/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7056 - loss: 0.9102\n",
      "Epoch 89/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7198 - loss: 0.8687\n",
      "Epoch 90/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7058 - loss: 0.9212\n",
      "Epoch 91/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.7241 - loss: 0.8748\n",
      "Epoch 92/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7231 - loss: 0.8816\n",
      "Epoch 93/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7021 - loss: 0.8926\n",
      "Epoch 94/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7030 - loss: 0.8792\n",
      "Epoch 95/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7187 - loss: 0.8641\n",
      "Epoch 96/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7029 - loss: 0.8970\n",
      "Epoch 97/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7188 - loss: 0.8607\n",
      "Epoch 98/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.7189 - loss: 0.8455\n",
      "Epoch 99/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6762 - loss: 0.9206\n",
      "Epoch 100/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7325 - loss: 0.8306\n",
      "Epoch 101/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7273 - loss: 0.8547\n",
      "Epoch 102/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7066 - loss: 0.8609\n",
      "Epoch 103/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.7102 - loss: 0.8325\n",
      "Epoch 104/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.7021 - loss: 0.8702\n",
      "Epoch 105/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6900 - loss: 0.8428\n",
      "Epoch 106/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7191 - loss: 0.8082\n",
      "Epoch 107/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7003 - loss: 0.8455\n",
      "Epoch 108/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7091 - loss: 0.8459\n",
      "Epoch 109/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.6809 - loss: 0.8729\n",
      "Epoch 110/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7125 - loss: 0.7892\n",
      "Epoch 111/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.6795 - loss: 0.8472\n",
      "Epoch 112/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7037 - loss: 0.8527\n",
      "Epoch 113/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6933 - loss: 0.8671\n",
      "Epoch 114/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7370 - loss: 0.7724\n",
      "Epoch 115/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7219 - loss: 0.7740\n",
      "Epoch 116/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7058 - loss: 0.8154\n",
      "Epoch 117/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7126 - loss: 0.8193\n",
      "Epoch 118/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7002 - loss: 0.8151\n",
      "Epoch 119/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7084 - loss: 0.7834\n",
      "Epoch 120/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.7088 - loss: 0.7862\n",
      "Epoch 121/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.6862 - loss: 0.8205\n",
      "Epoch 122/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7049 - loss: 0.8081\n",
      "Epoch 123/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7143 - loss: 0.7705\n",
      "Epoch 124/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7136 - loss: 0.7716\n",
      "Epoch 125/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7113 - loss: 0.7876\n",
      "Epoch 126/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6986 - loss: 0.7537\n",
      "Epoch 127/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7258 - loss: 0.7602\n",
      "Epoch 128/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6995 - loss: 0.7848\n",
      "Epoch 129/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.7095 - loss: 0.8014\n",
      "Epoch 130/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7409 - loss: 0.7388\n",
      "Epoch 131/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7035 - loss: 0.7941\n",
      "Epoch 132/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.7225 - loss: 0.7515\n",
      "Epoch 133/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7037 - loss: 0.7950\n",
      "Epoch 134/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7077 - loss: 0.7918\n",
      "Epoch 135/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7032 - loss: 0.8031\n",
      "Epoch 136/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7058 - loss: 0.7903\n",
      "Epoch 137/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7001 - loss: 0.7900\n",
      "Epoch 138/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7118 - loss: 0.7422\n",
      "Epoch 139/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7091 - loss: 0.7627\n",
      "Epoch 140/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7164 - loss: 0.7400\n",
      "Epoch 141/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6960 - loss: 0.7712\n",
      "Epoch 142/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7146 - loss: 0.7727\n",
      "Epoch 143/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7089 - loss: 0.7782\n",
      "Epoch 144/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7268 - loss: 0.7083\n",
      "Epoch 145/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7134 - loss: 0.7390\n",
      "Epoch 146/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7308 - loss: 0.7194\n",
      "Epoch 147/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7122 - loss: 0.7508\n",
      "Epoch 148/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7165 - loss: 0.7209\n",
      "Epoch 149/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7066 - loss: 0.7584\n",
      "Epoch 150/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6920 - loss: 0.7815\n",
      "Epoch 151/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6994 - loss: 0.7590\n",
      "Epoch 152/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.7101 - loss: 0.7507\n",
      "Epoch 153/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6933 - loss: 0.7577\n",
      "Epoch 154/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6954 - loss: 0.7652\n",
      "Epoch 155/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7345 - loss: 0.6951\n",
      "Epoch 156/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7333 - loss: 0.7033\n",
      "Epoch 157/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7060 - loss: 0.7435\n",
      "Epoch 158/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.7228 - loss: 0.7334\n",
      "Epoch 159/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.6911 - loss: 0.7570\n",
      "Epoch 160/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7037 - loss: 0.7365\n",
      "Epoch 161/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6937 - loss: 0.7540\n",
      "Epoch 162/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.7127 - loss: 0.7386\n",
      "Epoch 163/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.7136 - loss: 0.6989\n",
      "Epoch 164/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7132 - loss: 0.6956\n",
      "Epoch 165/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7179 - loss: 0.7346\n",
      "Epoch 166/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.7197 - loss: 0.7401\n",
      "Epoch 167/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.7006 - loss: 0.7260\n",
      "Epoch 168/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7067 - loss: 0.7316\n",
      "Epoch 169/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7011 - loss: 0.7433\n",
      "Epoch 170/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7161 - loss: 0.7250\n",
      "Epoch 171/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6922 - loss: 0.7574\n",
      "Epoch 172/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.7210 - loss: 0.7217\n",
      "Epoch 173/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7376 - loss: 0.6859\n",
      "Epoch 174/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6975 - loss: 0.7270\n",
      "Epoch 175/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.6982 - loss: 0.7311\n",
      "Epoch 176/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7084 - loss: 0.7329\n",
      "Epoch 177/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7255 - loss: 0.6812\n",
      "Epoch 178/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7120 - loss: 0.6930\n",
      "Epoch 179/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7112 - loss: 0.7248\n",
      "Epoch 180/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7135 - loss: 0.7059\n",
      "Epoch 181/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7166 - loss: 0.7136\n",
      "Epoch 182/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7089 - loss: 0.6953\n",
      "Epoch 183/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7292 - loss: 0.6850\n",
      "Epoch 184/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7196 - loss: 0.6968\n",
      "Epoch 185/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7305 - loss: 0.6650\n",
      "Epoch 186/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7224 - loss: 0.7121\n",
      "Epoch 187/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7160 - loss: 0.7123\n",
      "Epoch 188/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7475 - loss: 0.6434\n",
      "Epoch 189/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7005 - loss: 0.6913\n",
      "Epoch 190/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7061 - loss: 0.7017\n",
      "Epoch 191/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7271 - loss: 0.6675\n",
      "Epoch 192/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7147 - loss: 0.6970\n",
      "Epoch 193/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7058 - loss: 0.7192\n",
      "Epoch 194/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7208 - loss: 0.6875\n",
      "Epoch 195/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7014 - loss: 0.7021\n",
      "Epoch 196/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7102 - loss: 0.7148\n",
      "Epoch 197/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7114 - loss: 0.7243\n",
      "Epoch 198/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.7312 - loss: 0.6636\n",
      "Epoch 199/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7310 - loss: 0.6885\n",
      "Epoch 200/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7009 - loss: 0.7219\n",
      "Epoch 201/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7246 - loss: 0.6865\n",
      "Epoch 202/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7178 - loss: 0.6881\n",
      "Epoch 203/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.6878 - loss: 0.7230\n",
      "Epoch 204/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6909 - loss: 0.7563\n",
      "Epoch 205/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7285 - loss: 0.6559\n",
      "Epoch 206/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7008 - loss: 0.6901\n",
      "Epoch 207/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6892 - loss: 0.7049\n",
      "Epoch 208/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6967 - loss: 0.7165\n",
      "Epoch 209/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6967 - loss: 0.7154\n",
      "Epoch 210/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7229 - loss: 0.6803\n",
      "Epoch 211/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7196 - loss: 0.6776\n",
      "Epoch 212/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7095 - loss: 0.6821\n",
      "Epoch 213/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7053 - loss: 0.6740\n",
      "Epoch 214/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7222 - loss: 0.6821\n",
      "Epoch 215/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7203 - loss: 0.6606\n",
      "Epoch 216/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.6984 - loss: 0.7023\n",
      "Epoch 217/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6968 - loss: 0.6922\n",
      "Epoch 218/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6963 - loss: 0.6859\n",
      "Epoch 219/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7408 - loss: 0.6773\n",
      "Epoch 220/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7197 - loss: 0.6975\n",
      "Epoch 221/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6944 - loss: 0.7280\n",
      "Epoch 222/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7116 - loss: 0.6936\n",
      "Epoch 223/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.6943 - loss: 0.6861\n",
      "Epoch 224/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6901 - loss: 0.7022\n",
      "Epoch 225/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7078 - loss: 0.6871\n",
      "Epoch 226/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7117 - loss: 0.6798\n",
      "Epoch 227/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7056 - loss: 0.6796\n",
      "Epoch 228/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7203 - loss: 0.6729\n",
      "Epoch 229/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7013 - loss: 0.6922\n",
      "Epoch 230/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7239 - loss: 0.6664\n",
      "Epoch 231/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7274 - loss: 0.6656\n",
      "Epoch 232/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7247 - loss: 0.6782\n",
      "Epoch 233/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7108 - loss: 0.6607\n",
      "Epoch 234/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7120 - loss: 0.6636\n",
      "Epoch 235/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7343 - loss: 0.6851\n",
      "Epoch 236/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7196 - loss: 0.6570\n",
      "Epoch 237/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.7121 - loss: 0.6956\n",
      "Epoch 238/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7130 - loss: 0.6634\n",
      "Epoch 239/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7217 - loss: 0.6661\n",
      "Epoch 240/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7023 - loss: 0.6885\n",
      "Epoch 241/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6849 - loss: 0.7162\n",
      "Epoch 242/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7219 - loss: 0.6764\n",
      "Epoch 243/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7349 - loss: 0.6531\n",
      "Epoch 244/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7226 - loss: 0.6812\n",
      "Epoch 245/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7014 - loss: 0.6809\n",
      "Epoch 246/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.7247 - loss: 0.6708\n",
      "Epoch 247/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7133 - loss: 0.6595\n",
      "Epoch 248/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7096 - loss: 0.6751\n",
      "Epoch 249/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.7137 - loss: 0.6694\n",
      "Epoch 250/250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6997 - loss: 0.6777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x298c8d76d80>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(source_sequences, target_sequences_one_hot,batch_size = 64, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c136c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter String :  Thanks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Input Sequence: Thanks\n",
      "Translated Sequence: Gracias!\n"
     ]
    }
   ],
   "source": [
    "# Translate a new input sequence\n",
    "x = input(\"Enter String : \")\n",
    "input_sequence = x\n",
    "input_sequence = [source_char_to_int[char] for char in input_sequence]\n",
    "input_sequence = tf.keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=max_sequence_length, padding='post')\n",
    "output_sequence = model.predict(input_sequence)[0]\n",
    "# Decode the output sequence\n",
    "output_sequence = [target_int_to_char[np.argmax(char)] for char in output_sequence]\n",
    "print(\"Input Sequence:\",x)\n",
    "print(\"Translated Sequence:\", ''.join(output_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f776f9",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;background-color:teal\">"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5923474,
     "sourceId": 9689378,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
