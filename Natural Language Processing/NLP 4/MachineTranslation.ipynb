{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zENcxjEgujKJ",
        "outputId": "362ebde0-1e55-4348-8b20-26452b0a73f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 6s 6s/step - loss: 1.7966\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.7062\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 1.6074\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.4785\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 1.2971\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 1.0567\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.8632\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.8974\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.8852\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.7900\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.6897\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.6299\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.6176\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.6267\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.6141\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.5719\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.5219\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.4834\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.4616\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.4495\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.4351\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.4095\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.3753\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.3472\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.3378\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.3338\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.3129\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.2845\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.2683\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.2653\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 0.2588\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.2416\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.2245\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.2164\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.2073\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1906\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.1772\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.1718\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 170ms/step - loss: 0.1637\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.1494\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.1374\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.1292\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.1183\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.1055\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0958\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0875\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0775\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0669\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0581\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.0528\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0515\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0513\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0482\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0409\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0331\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0279\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0248\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0227\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0208\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0191\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0175\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.0160\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0144\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0127\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0112\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0100\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0089\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0080\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0072\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0066\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0061\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0058\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0055\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0052\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0049\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0046\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0043\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0041\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0038\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0036\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0034\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0032\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0030\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0029\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0027\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0026\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0025\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0024\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0023\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0023\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0022\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0021\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0021\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0020\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0019\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0019\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0018\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0018\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0017\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0017\n",
            "1/1 [==============================] - 0s 482ms/step\n",
            "1/1 [==============================] - 0s 488ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Input Sentence: hello how are you\n",
            "Translation: salut comment ca va <START> <START> <START> <START> <START> <START> <START>\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Define the input and target language vocabularies\n",
        "input_vocab = ['hello', 'how', 'are', 'you']\n",
        "target_vocab = ['<START>', '<END>','salut', 'comment', 'ca', 'va']\n",
        "\n",
        "# Define the maximum sequence length for input and target sentences\n",
        "max_seq_length = 10\n",
        "\n",
        "# Create dictionaries to map words to indices and vice versa\n",
        "input_word2idx = {word: idx for idx, word in enumerate(input_vocab)}\n",
        "input_idx2word = {idx: word for idx, word in enumerate(input_vocab)}\n",
        "target_word2idx = {word: idx for idx, word in enumerate(target_vocab)}\n",
        "target_idx2word = {idx: word for idx, word in enumerate(target_vocab)}\n",
        "\n",
        "# Define the encoder model\n",
        "encoder_input = Input(shape=(None,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(len(input_vocab), 256)(encoder_input)\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define the decoder model\n",
        "decoder_input = Input(shape=(None,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(len(target_vocab), 256)(decoder_input)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(target_vocab), activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Create the model\n",
        "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Generate training data\n",
        "input_sentences = ['hello how are you', 'how are you']\n",
        "target_sentences = ['salut comment ca va', 'comment ca va']\n",
        "encoder_input_data = np.zeros((len(input_sentences), max_seq_length), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(target_sentences), max_seq_length), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(target_sentences), max_seq_length), dtype='float32')\n",
        "\n",
        "for i, (input_sentence, target_sentence) in enumerate(zip(input_sentences, target_sentences)):\n",
        "    for t, word in enumerate(input_sentence.split()):\n",
        "        encoder_input_data[i, t] = input_word2idx[word]\n",
        "    for t, word in enumerate(target_sentence.split()):\n",
        "        if t == 0:\n",
        "            decoder_input_data[i, t] = target_word2idx['<START>']\n",
        "        else:\n",
        "            decoder_input_data[i, t] = target_word2idx[word]\n",
        "        decoder_target_data[i, t] = target_word2idx[word]\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=2, epochs=100)\n",
        "\n",
        "# Define the inference models\n",
        "encoder_model = Model(encoder_input, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model([decoder_input] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Translate a sentence\n",
        "def translate_sentence(input_sentence):\n",
        "    input_tokens = [input_word2idx[word] for word in input_sentence.split()]\n",
        "    input_seq = tf.keras.preprocessing.sequence.pad_sequences([input_tokens], maxlen=max_seq_length, padding='post')\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    \n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_word2idx['<START>']\n",
        "    \n",
        "    stop_condition = False\n",
        "    translated_sentence = ''\n",
        "    \n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = target_idx2word[sampled_token_index]\n",
        "        translated_sentence += ' ' + sampled_word\n",
        "        \n",
        "        if sampled_word == '<END>' or len(translated_sentence.split()) > max_seq_length:\n",
        "            stop_condition = True\n",
        "        \n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        \n",
        "        states_value = [h, c]\n",
        "        \n",
        "    return translated_sentence.strip()\n",
        "\n",
        "# Test the translation function\n",
        "input_sentence = 'hello how are you'\n",
        "translation = translate_sentence(input_sentence)\n",
        "print('Input Sentence:', input_sentence)\n",
        "print('Translation:', translation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8OQagR9u1GY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}